{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP Básico con Spacy\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/Ohtar10/icesi-nlp/blob/main/Sesion1/1-spacy-basics.ipynb)\n",
    "\n",
    "## Referencias\n",
    "* [NLP - Natural Language Processing With Python](https://www.udemy.com/course/nlp-natural-language-processing-with-python)\n",
    "* [Natural Language Processing in Action](https://www.manning.com/books/natural-language-processing-in-action)\n",
    "\n",
    "Este notebook contiene ejemplos básico de uso de la librería Spacy para procesamiento de lenguaje natural con técnicas clásicas. Esta herramienta nos servirá para familiarizarnos con los métodos clásicos.\n",
    "\n",
    "## Preparación del entorno\n",
    "Asumiendo que la librería ya se encuentra instalada, dependiendo de la tarea, necesitamos descargar un corpus, por ejemplo en el idioma ingles sería:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1317889/2396000874.py:1: DeprecationWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html\n",
      "  import pkg_resources\n"
     ]
    }
   ],
   "source": [
    "import pkg_resources\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "installed_packages = [package.key for package in pkg_resources.working_set]\n",
    "IN_COLAB = 'google-colab' in installed_packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "!test '{IN_COLAB}' = 'True' && wget  https://github.com/Ohtar10/icesi-nlp/raw/refs/heads/main/requirements.txt && pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting en-core-web-sm==3.8.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.8.0/en_core_web_sm-3.8.0-py3-none-any.whl (12.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m37.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: en-core-web-sm\n",
      "Successfully installed en-core-web-sm-3.8.0\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El cual debemos luego importar:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "# load the simplified version of the english core language\n",
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creando un documento simple\n",
    "Este documento será automáticamente interpretado con spacy para el lenguaje seleccionado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(u'Tesla is looking at buying U.S. startup for $6 million')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Desde aquí, podemos observar los diferentes elementos del documento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token               POS                 S-dep               \n",
      "Tesla               PROPN               nsubj\n",
      "is                  AUX                 aux\n",
      "looking             VERB                ROOT\n",
      "at                  ADP                 prep\n",
      "buying              VERB                pcomp\n",
      "U.S.                PROPN               dobj\n",
      "startup             VERB                advcl\n",
      "for                 ADP                 prep\n",
      "$                   SYM                 quantmod\n",
      "6                   NUM                 compound\n",
      "million             NUM                 pobj\n"
     ]
    }
   ],
   "source": [
    "col1 = \"Token\"\n",
    "col2 = \"POS\" # Part of Speech\n",
    "col3 = \"S-dep\" # Syntactic dependency\n",
    "\n",
    "print(f\"{col1:{20}}{col2:{20}}{col3:{20}}\")\n",
    "for token in doc:\n",
    "    print(f\"{token.text:{20}}{token.pos_:{20}}{token.dep_}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hemos impreso los tokens (palabras en este caso), la parte del contexto que representan (POS) y la dependencia semantica que dicho token tiene.\n",
    "\n",
    "En NLP clásico hay una taxonomía especializada para cada elemento del lenguaje. Cada elemento fue producto de estudios diversos y variados con el fin de ofrecer un modelado sistemático del lenguaje. Expertos en lenguaje estuvieron involucrados en la creación de esta taxonomía.\n",
    "\n",
    "Ahora, librerías como Spacy facilitan el procesamiento de esta taxonomía."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Un pipeline simple de Spacy\n",
    "\n",
    "El núcleo de Spacy es el pipeline que no es más que el procesamiento/transformación que toma el texto original y se lo somete a diversos procesos de NLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('tok2vec', <spacy.pipeline.tok2vec.Tok2Vec at 0x700687847340>),\n",
       " ('tagger', <spacy.pipeline.tagger.Tagger at 0x700687847a00>),\n",
       " ('parser', <spacy.pipeline.dep_parser.DependencyParser at 0x7005b03cb370>),\n",
       " ('attribute_ruler',\n",
       "  <spacy.pipeline.attributeruler.AttributeRuler at 0x7006877c7a00>),\n",
       " ('lemmatizer',\n",
       "  <spacy.lang.en.lemmatizer.EnglishLemmatizer at 0x7005ab9f6a00>),\n",
       " ('ner', <spacy.pipeline.ner.EntityRecognizer at 0x7006879d31b0>)]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp.pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como podemos observar aquí, la instanciación por defecto es un pipeline compuesto por diferentes componentes que deberían ser familiares para nosotros:\n",
    "\n",
    "* Token 2 Vec: Convertir los tokens en vectores.\n",
    "* Lemmatizer: Extracción de componentes raíz de las palabras\n",
    "* NER: Named entity recognition para identificar los sujetos de los documentos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Un documento es iterable y los items pueden ser accedidos por índice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The 0th token in the document is: Tesla\n"
     ]
    }
   ],
   "source": [
    "n = 0\n",
    "print(f\"The {n}th token in the document is: {doc[n]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploremos diferentes elementos transformados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.tokens.doc import Doc\n",
    "import pandas as pd\n",
    "\n",
    "def get_doc_elements(doc: Doc):\n",
    "    elements = [\"text\", \"lemma\", \"pos\", \"tag\", \"shape\", \"alpha\", \"stop\"]\n",
    "    rows = [ [token.text, token.lemma_, token.pos_, token.tag_, token.shape_, token.is_alpha, token.is_stop] \n",
    "            for token in  doc]\n",
    "    return pd.DataFrame(rows, columns=elements)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>lemma</th>\n",
       "      <th>pos</th>\n",
       "      <th>tag</th>\n",
       "      <th>shape</th>\n",
       "      <th>alpha</th>\n",
       "      <th>stop</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Tesla</td>\n",
       "      <td>Tesla</td>\n",
       "      <td>PROPN</td>\n",
       "      <td>NNP</td>\n",
       "      <td>Xxxxx</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>is</td>\n",
       "      <td>be</td>\n",
       "      <td>AUX</td>\n",
       "      <td>VBZ</td>\n",
       "      <td>xx</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>looking</td>\n",
       "      <td>look</td>\n",
       "      <td>VERB</td>\n",
       "      <td>VBG</td>\n",
       "      <td>xxxx</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>at</td>\n",
       "      <td>at</td>\n",
       "      <td>ADP</td>\n",
       "      <td>IN</td>\n",
       "      <td>xx</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>buying</td>\n",
       "      <td>buy</td>\n",
       "      <td>VERB</td>\n",
       "      <td>VBG</td>\n",
       "      <td>xxxx</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>U.S.</td>\n",
       "      <td>U.S.</td>\n",
       "      <td>PROPN</td>\n",
       "      <td>NNP</td>\n",
       "      <td>X.X.</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>startup</td>\n",
       "      <td>startup</td>\n",
       "      <td>VERB</td>\n",
       "      <td>VBD</td>\n",
       "      <td>xxxx</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>for</td>\n",
       "      <td>for</td>\n",
       "      <td>ADP</td>\n",
       "      <td>IN</td>\n",
       "      <td>xxx</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>$</td>\n",
       "      <td>$</td>\n",
       "      <td>SYM</td>\n",
       "      <td>$</td>\n",
       "      <td>$</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>NUM</td>\n",
       "      <td>CD</td>\n",
       "      <td>d</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>million</td>\n",
       "      <td>million</td>\n",
       "      <td>NUM</td>\n",
       "      <td>CD</td>\n",
       "      <td>xxxx</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       text    lemma    pos  tag  shape  alpha   stop\n",
       "0     Tesla    Tesla  PROPN  NNP  Xxxxx   True  False\n",
       "1        is       be    AUX  VBZ     xx   True   True\n",
       "2   looking     look   VERB  VBG   xxxx   True  False\n",
       "3        at       at    ADP   IN     xx   True   True\n",
       "4    buying      buy   VERB  VBG   xxxx   True  False\n",
       "5      U.S.     U.S.  PROPN  NNP   X.X.  False  False\n",
       "6   startup  startup   VERB  VBD   xxxx   True  False\n",
       "7       for      for    ADP   IN    xxx   True   True\n",
       "8         $        $    SYM    $      $  False  False\n",
       "9         6        6    NUM   CD      d  False  False\n",
       "10  million  million    NUM   CD   xxxx   True  False"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_elements = get_doc_elements(doc)\n",
    "doc_elements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Done:\n",
    "\n",
    "|Tag|Descrición|doc2[0].tag|\n",
    "|:------|:------:|:------|\n",
    "|`.text`|The original word text<!-- .element: style=\"text-align:left;\" -->|`Tesla`|\n",
    "|`.lemma_`|The base form of the word|`tesla`|\n",
    "|`.pos_`|The simple part-of-speech tag|`PROPN`/`proper noun`|\n",
    "|`.tag_`|The detailed part-of-speech tag|`NNP`/`noun, proper singular`|\n",
    "|`.shape_`|The word shape – capitalization, punctuation, digits|`Xxxxx`|\n",
    "|`.is_alpha`|Is the token an alpha character?|`True`|\n",
    "|`.is_stop`|Is the token part of a stop list, i.e. the most common words of the language?|`False`|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objetos Span\n",
    "Un span puede interpretarse como una porción de un documento, es decir, puede empezar desde alún índice hasta otro. Esto facilita el procesamiento por pedazos (chunks) en lugar el documento completo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "computer science, information engineering, and artificial intelligence concerned with the interactions between computers and human (natural)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Definition of NLP according to Wikipedia \n",
    "doc = nlp(u\"Natural language processing (NLP) is a subfield of computer science, \\\n",
    "information engineering, and artificial intelligence concerned with the \\\n",
    "interactions between computers and human (natural) languages, in particular \\\n",
    "how to program computers to process and analyze large amounts of natural language data.\\\n",
    "Challenges in natural language processing frequently involve speech recognition, natural \\\n",
    "language understanding, and natural language generation.\")\n",
    "\n",
    "quote = doc[10:30]\n",
    "quote"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observemos aquí que el slice es por los tokens y no por los caracteres individuales. Esto es muy útil ya que podemos estar seguros de no interrumpir abruptamente los tokens."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trabajando con oraciones\n",
    "Podemos iterar sobre oraciones en los documentos, es decir, frases separadas por el punto \".\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is the first sentence.\n",
      "This is the second sentence.\n",
      "And this is the last sentence.\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(\"This is the first sentence. This is the second sentence. And this is the last sentence.\")\n",
    "for sent in doc.sents:\n",
    "    print(sent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Nota:** Cada punto es considerado un token, etnonces en el segundo \"This\" en el anterior documento está en el índice `6`, no en el `5`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token 5: .\n",
      "Token 6: This\n",
      "Is token 6 a sentence start? True\n"
     ]
    }
   ],
   "source": [
    "print(f\"Token 5: {doc[5]}\")\n",
    "print(f\"Token 6: {doc[6]}\")\n",
    "print(f\"Is token 6 a sentence start? {doc[6].is_sent_start}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "icesi-nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "undefined.undefined.undefined"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
