{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vocabulario y Matching\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/Ohtar10/icesi-nlp/blob/main/Sesion1/5-vocabulary.ipynb)\n",
    "\n",
    "## Referencias\n",
    "* [NLP - Natural Language Processing With Python](https://www.udemy.com/course/nlp-natural-language-processing-with-python)\n",
    "* [Natural Language Processing in Action](https://www.manning.com/books/natural-language-processing-in-action)\n",
    "\n",
    "Ahora miremos algunos ejemplos y pruebas con vocabularios en Spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1565021/2396000874.py:1: DeprecationWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html\n",
      "  import pkg_resources\n"
     ]
    }
   ],
   "source": [
    "import pkg_resources\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "installed_packages = [package.key for package in pkg_resources.working_set]\n",
    "IN_COLAB = 'google-colab' in installed_packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "!test '{IN_COLAB}' = 'True' && wget  https://github.com/Ohtar10/icesi-nlp/raw/refs/heads/main/requirements.txt && pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Matchers\n",
    "Podemos pensar en los matchers de forma similar a como usamos las expresiones regulares en programación tradicional, solo que aplicado a documentos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy.matcher import Matcher\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "matcher = Matcher(nlp.vocab) # work with the normal vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SolarPower\n",
    "pattern1 = [{'LOWER': 'solarpower'}]\n",
    "# Solar-power\n",
    "pattern2 = [{'LOWER': 'solar'}, {'IS_PUNCT': True}, {'LOWER': 'power'}]\n",
    "# Solar power\n",
    "pattern3 = [{'LOWER': 'solar'}, {'LOWER': 'power'}]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observemos el ejemplo anterior y los correspondientes patrones que queremos encontrar. Nótese que cada parte puede ser representada como un diccionario. Esto también es una forma de hacer *tagging* de los tokens, más allá de los estándar POS, que pueden ser útiles dependiendo de la tarea en cuestión.\n",
    "\n",
    "## Otros atributos para los tokens\n",
    "A parte de los lemmas, hay otra variedad de atributos para los tokens que podemos utilizar para determinar reglas de matching:\n",
    "\n",
    "<table><tr><th>Attribute</th><th>Description</th></tr>\n",
    "\n",
    "<tr ><td><span >`ORTH`</span></td><td>The exact verbatim text of a token</td></tr>\n",
    "<tr ><td><span >`LOWER`</span></td><td>The lowercase form of the token text</td></tr>\n",
    "<tr ><td><span >`LENGTH`</span></td><td>The length of the token text</td></tr>\n",
    "<tr ><td><span >`IS_ALPHA`, `IS_ASCII`, `IS_DIGIT`</span></td><td>Token text consists of alphanumeric characters, ASCII characters, digits</td></tr>\n",
    "<tr ><td><span >`IS_LOWER`, `IS_UPPER`, `IS_TITLE`</span></td><td>Token text is in lowercase, uppercase, titlecase</td></tr>\n",
    "<tr ><td><span >`IS_PUNCT`, `IS_SPACE`, `IS_STOP`</span></td><td>Token is punctuation, whitespace, stop word</td></tr>\n",
    "<tr ><td><span >`LIKE_NUM`, `LIKE_URL`, `LIKE_EMAIL`</span></td><td>Token text resembles a number, URL, email</td></tr>\n",
    "<tr ><td><span >`POS`, `TAG`, `DEP`, `LEMMA`, `SHAPE`</span></td><td>The token's simple and extended part-of-speech tag, dependency label, lemma, shape</td></tr>\n",
    "<tr ><td><span >`ENT_TYPE`</span></td><td>The token's entity label</td></tr>\n",
    "\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora podemos agregar estos patrones al matcher y aplicarlos a un documento:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "patterns = [pattern1, pattern2, pattern3]\n",
    "matcher.add('SolarPower', patterns)\n",
    "doc = nlp('The Solar Power industry continues to grow as solarpower increases. \\\n",
    "Solar-power cars are gaining popularity. ')\n",
    "\n",
    "found_matches = matcher(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(8656102463236116519, 1, 3),\n",
       " (8656102463236116519, 8, 9),\n",
       " (8656102463236116519, 11, 14)]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "found_matches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La lista obtenida contiene los ID de los tokens y los índices desde donde comienzan y terminan dentro del documento. Podemos imprimir esta información utilizando el vocabulario con los ID de token que hemos obtenido."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8656102463236116519 SolarPower 1 3 Solar Power\n",
      "8656102463236116519 SolarPower 8 9 solarpower\n",
      "8656102463236116519 SolarPower 11 14 Solar-power\n"
     ]
    }
   ],
   "source": [
    "def print_matches(found_matches):\n",
    "    for match_id, start, end in found_matches:\n",
    "        string_id = nlp.vocab.strings[match_id] # get string representation\n",
    "        span = doc[start:end] # get a span of this particular match\n",
    "        print(match_id, string_id, start, end, span.text)\n",
    "\n",
    "print_matches(found_matches)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aquí podemos observar diferentes marches que apuntan a la misma entrada en el vocabulario (el matcher) y el correspondiente texto al que hace referencia.\n",
    "\n",
    "Podemos remover los matchers si ya no nos interesa trabajar cone llos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "matcher.remove('SolarPower') # use the name we defined when we added it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Y agregar otro conjunto de marchers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# solarpower SolarPower\n",
    "pattern1 = [{'LOWER': 'solarpower'}]\n",
    "# solar.power solar-power etc\n",
    "pattern2 = [{'LOWER': 'solar'}, {'IS_PUNCT': True, 'OP': '*'}, {'LOWER': 'power'}]\n",
    "\n",
    "patterns = [pattern1, pattern2]\n",
    "matcher.add('SolarPower', patterns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8656102463236116519 SolarPower 1 3 Solar Power\n",
      "8656102463236116519 SolarPower 8 9 solarpower\n",
      "8656102463236116519 SolarPower 11 14 Solar-power\n"
     ]
    }
   ],
   "source": [
    "found_matches = matcher(doc)\n",
    "print_matches(found_matches)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Los cuantificadores a continuación pueden ser procesados con el operador correspondiente:\n",
    "<table><tr><th>OP</th><th>Description</th></tr>\n",
    "\n",
    "<tr ><td><span >\\!</span></td><td>Negate the pattern, by requiring it to match exactly 0 times</td></tr>\n",
    "<tr ><td><span >?</span></td><td>Make the pattern optional, by allowing it to match 0 or 1 times</td></tr>\n",
    "<tr ><td><span >\\+</span></td><td>Require the pattern to match 1 or more times</td></tr>\n",
    "<tr ><td><span >\\*</span></td><td>Allow the pattern to match zero or more times</td></tr>\n",
    "</table>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notemos que hemos obtenido el mismo resultado pero solo con un patrón menos, esto es gracias a la opción `OP` en el segundo patrón la cual hace que, al igual que en regex, haga match con cero o más ocurrencias."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Matchers de frases\n",
    "Como otra alternativa para la construcción de patrones, podemos crear unos con frases:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.matcher import PhraseMatcher\n",
    "\n",
    "matcher = PhraseMatcher(nlp.vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Empecemos a trabajar con un corpus real. Vamos a usar el extracto del articulo de *Reaganomics* de la Wikipedia:\n",
    "\n",
    "* https://en.wikipedia.org/wiki/Reaganomics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "!test '{IN_COLAB}' = 'True' && wget  https://github.com/Ohtar10/icesi-nlp/raw/refs/heads/main/Sesion1/reaganomics.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./reaganomics.txt', encoding='ISO-8859-1') as file:\n",
    "    doc = nlp(file.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Y luego vamos a buscar patrones de frases como:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "phrase_list = ['voodoo economics', 'supply-side economics', 'trickle-down economics', 'free-market economics']\n",
    "phrase_patterns = [nlp(text) for text in phrase_list]\n",
    "matcher.add('EconMatcher', None, *phrase_patterns)\n",
    "found_matches = matcher(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3680293220734633682 EconMatcher 41 45 supply-side economics\n",
      "3680293220734633682 EconMatcher 49 53 trickle-down economics\n",
      "3680293220734633682 EconMatcher 54 56 voodoo economics\n",
      "3680293220734633682 EconMatcher 61 65 free-market economics\n",
      "3680293220734633682 EconMatcher 673 677 supply-side economics\n",
      "3680293220734633682 EconMatcher 2986 2990 trickle-down economics\n"
     ]
    }
   ],
   "source": [
    "print_matches(found_matches)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hemos encontrado una serie de matches, con sus respectivos índices de donde se encuentran dichas frases.\n",
    "\n",
    "Esto puede ser útil a la hora de construir vocabularios, recordemos que en NLP, los vocabularios no están limitados exclusivamente a palabras sueltas, muchas veces dos palabras juntas traen un significado diferente que si estuvieran separadas y valdría la pena considerarlas como un token independiente."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "icesi-nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
