{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lemmatization\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/Ohtar10/icesi-nlp/blob/main/Sesion1/4-lemmatization.ipynb)\n",
    "\n",
    "## Referencias\n",
    "* [NLP - Natural Language Processing With Python](https://www.udemy.com/course/nlp-natural-language-processing-with-python)\n",
    "* [Natural Language Processing in Action](https://www.manning.com/books/natural-language-processing-in-action)\n",
    "\n",
    "Lemmatization va más allá del stemmin al hacer un análisis morfológico de las palabras y encontrar raices comunes entre diferentes palabras no al usar la palabra común más larga sino la raíz semántica real de la palabra. For ejemplo, el *lemma* de la palabra *was* (en inglés, pasado de ser o estar) es precisamente *ser* ya que es el mismo verbo pero en diferente tiempo, lo cual es mucho más útil que simplemente trocear las palabras.\n",
    "\n",
    "Para este caso, volverémos a usar Spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1492104/2396000874.py:1: DeprecationWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html\n",
      "  import pkg_resources\n"
     ]
    }
   ],
   "source": [
    "import pkg_resources\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "installed_packages = [package.key for package in pkg_resources.working_set]\n",
    "IN_COLAB = 'google-colab' in installed_packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "!test '{IN_COLAB}' = 'True' && wget  https://github.com/Ohtar10/icesi-nlp/raw/refs/heads/main/requirements.txt && pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analicemos la siguiente oración:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "doc = nlp(\"I am a runner running in a race because I love to run since I ran today\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I                   PRON                4690420944186131903 \tI                             \n",
      "am                  AUX                 10382539506755952630\tbe                            \n",
      "a                   DET                 11901859001352538922\ta                             \n",
      "runner              NOUN                12640964157389618806\trunner                        \n",
      "running             VERB                12767647472892411841\trun                           \n",
      "in                  ADP                 3002984154512732771 \tin                            \n",
      "a                   DET                 11901859001352538922\ta                             \n",
      "race                NOUN                8048469955494714898 \trace                          \n",
      "because             SCONJ               16950148841647037698\tbecause                       \n",
      "I                   PRON                4690420944186131903 \tI                             \n",
      "love                VERB                3702023516439754181 \tlove                          \n",
      "to                  PART                3791531372978436496 \tto                            \n",
      "run                 VERB                12767647472892411841\trun                           \n",
      "since               SCONJ               10066841407251338481\tsince                         \n",
      "I                   PRON                4690420944186131903 \tI                             \n",
      "ran                 VERB                12767647472892411841\trun                           \n",
      "today               NOUN                11042482332948150395\ttoday                         \n"
     ]
    }
   ],
   "source": [
    "for token in doc:\n",
    "    print(f\"{token.text:{20}}{token.pos_:{20}}{token.lemma:<{20}}\\t{token.lemma_:{30}}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observamos que para cada token obtenemos el POS (Part of Speech) y su correspondiente lemma. Nótese que los verbos son correctamente lemmatizados a su raíz."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "icesi-nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
