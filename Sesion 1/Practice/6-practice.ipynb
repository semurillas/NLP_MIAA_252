{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z_qR5xTubPPN"
      },
      "source": [
        "# NLP Basics Assessment"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "[![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/semurillas/NLP_MIAA_252/blob/main/Sesion%201/Practice/6-practice.ipynb)\n",
        "\n",
        "\n",
        "<center>\n",
        "\n",
        "<h1>ğŸ“š MaestrÃ­a en Inteligencia Artificial Aplicada â€“ 3er Semestre</h1>\n",
        "\n",
        "<h3>Asignatura: Procesamiento de Lenguaje Natural</h3>\n",
        "\n",
        "<hr style=\"width:60%;\">\n",
        "\n",
        "<h2>ğŸ‘¨â€ğŸ“ Estudiantes</h2>\n",
        "<ul style=\"list-style:none; padding:0; font-size:18px;\">\n",
        "    <li>Claudia MartÃ­nez</li>\n",
        "    <li>SebastiÃ¡n Murillas</li>\n",
        "    <li>Mario J. Castellanos</li>\n",
        "    <li>Enrique Manzano</li>\n",
        "    <li>Octavio Guerra</li>\n",
        "</ul>\n",
        "\n",
        "<hr style=\"width:60%;\">\n",
        "\n",
        "<h3>ğŸ“… Fecha: Agosto 16, 2025</h3>\n",
        "\n",
        "</center>\n"
      ],
      "metadata": {
        "id": "ovt7LOQr8Slr"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6vAnJ-Z2bPPP"
      },
      "source": [
        "En este notebook vamos a poner en prÃ¡ctica algunos de los conceptos vistos en los notebooks anteriores (SecciÃ³n Learning en Repositorio GitHub), aplicado a un corpus especÃ­fico que cada uno de los estudiantes del grupo ha seleccionado.\n",
        "\n",
        "## Referencias\n",
        "* [NLP - Natural Language Processing With Python](https://www.udemy.com/course/nlp-natural-language-processing-with-python)\n",
        "* [Natural Language Processing in Action](https://www.manning.com/books/natural-language-processing-in-action)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Chequeo de ambiente y Manejo de \"Warnings\"\n",
        "\n",
        "En esta primer cÃ³digo verificamos si el ambiente de Trabajo es \"Google Colab\" e ignorar algun mensaje de \"Warning\" que suceda a lo resto del Notebook."
      ],
      "metadata": {
        "id": "TtMCAuBEkbYH"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "wtYAN6nKbPPQ"
      },
      "outputs": [],
      "source": [
        "import importlib.metadata\n",
        "import warnings\n",
        "\n",
        "installed_packages = [dist.metadata['Name'].lower() for dist in importlib.metadata.Distribution.discover()]\n",
        "IN_COLAB = 'google-colab' in installed_packages\n",
        "\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Cargue de LibrerÃ­as\n",
        "\n",
        "Confirmando a traves de validaciÃ³n que estamos trabajando en \"Google Colab\", instalamos las librerÃ­as requeridas en el Notebook usando: **pip install -r** y el Archivo de Requerimientos desde el GitHub del Proyecto:\n",
        "\n",
        "https://raw.githubusercontent.com/semurillas/NLP_MIAA_252/refs/heads/main/requirements.txt\n",
        "\n",
        "###Notas:\n",
        "- Agregamos al URL el texto: **$(date +%s)**, para garantizar que se use la Ãºltima versiÃ³n del Archivo requerimients.txt en el Notebook.\n",
        "- En el cargue con PIP esta incluida **SPACY** en el archivo requeriments.txt"
      ],
      "metadata": {
        "id": "pESu81rxld8o"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TapM8U6wbPPR",
        "outputId": "09d41c07-f4a0-40af-d3dd-3bdb6424f3fc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pip in /usr/local/lib/python3.11/dist-packages (from -r https://raw.githubusercontent.com/semurillas/NLP_MIAA_252/refs/heads/main/requirements.txt?nocache=1755386267 (line 2)) (24.1.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from -r https://raw.githubusercontent.com/semurillas/NLP_MIAA_252/refs/heads/main/requirements.txt?nocache=1755386267 (line 3)) (75.2.0)\n",
            "Requirement already satisfied: wheel in /usr/local/lib/python3.11/dist-packages (from -r https://raw.githubusercontent.com/semurillas/NLP_MIAA_252/refs/heads/main/requirements.txt?nocache=1755386267 (line 4)) (0.45.1)\n",
            "Requirement already satisfied: spacy<3.9,>=3.8 in /usr/local/lib/python3.11/dist-packages (from -r https://raw.githubusercontent.com/semurillas/NLP_MIAA_252/refs/heads/main/requirements.txt?nocache=1755386267 (line 7)) (3.8.7)\n",
            "Requirement already satisfied: nltk>=3.9 in /usr/local/lib/python3.11/dist-packages (from -r https://raw.githubusercontent.com/semurillas/NLP_MIAA_252/refs/heads/main/requirements.txt?nocache=1755386267 (line 8)) (3.9.1)\n",
            "Requirement already satisfied: transformers>=4.41 in /usr/local/lib/python3.11/dist-packages (from transformers[torch]>=4.41->-r https://raw.githubusercontent.com/semurillas/NLP_MIAA_252/refs/heads/main/requirements.txt?nocache=1755386267 (line 9)) (4.55.1)\n",
            "Requirement already satisfied: datasets>=2.19 in /usr/local/lib/python3.11/dist-packages (from -r https://raw.githubusercontent.com/semurillas/NLP_MIAA_252/refs/heads/main/requirements.txt?nocache=1755386267 (line 10)) (4.0.0)\n",
            "Requirement already satisfied: sentence-transformers>=3.0 in /usr/local/lib/python3.11/dist-packages (from -r https://raw.githubusercontent.com/semurillas/NLP_MIAA_252/refs/heads/main/requirements.txt?nocache=1755386267 (line 11)) (5.1.0)\n",
            "Requirement already satisfied: torch>=2.6 in /usr/local/lib/python3.11/dist-packages (from -r https://raw.githubusercontent.com/semurillas/NLP_MIAA_252/refs/heads/main/requirements.txt?nocache=1755386267 (line 14)) (2.6.0+cu124)\n",
            "Requirement already satisfied: torchvision>=0.21 in /usr/local/lib/python3.11/dist-packages (from -r https://raw.githubusercontent.com/semurillas/NLP_MIAA_252/refs/heads/main/requirements.txt?nocache=1755386267 (line 15)) (0.21.0+cu124)\n",
            "Requirement already satisfied: torchaudio>=2.6 in /usr/local/lib/python3.11/dist-packages (from -r https://raw.githubusercontent.com/semurillas/NLP_MIAA_252/refs/heads/main/requirements.txt?nocache=1755386267 (line 16)) (2.6.0+cu124)\n",
            "Collecting lightning>=2.2 (from -r https://raw.githubusercontent.com/semurillas/NLP_MIAA_252/refs/heads/main/requirements.txt?nocache=1755386267 (line 17))\n",
            "  Downloading lightning-2.5.3-py3-none-any.whl.metadata (39 kB)\n",
            "Requirement already satisfied: tensorboard==2.19.0 in /usr/local/lib/python3.11/dist-packages (from -r https://raw.githubusercontent.com/semurillas/NLP_MIAA_252/refs/heads/main/requirements.txt?nocache=1755386267 (line 18)) (2.19.0)\n",
            "Requirement already satisfied: accelerate>=0.30 in /usr/local/lib/python3.11/dist-packages (from -r https://raw.githubusercontent.com/semurillas/NLP_MIAA_252/refs/heads/main/requirements.txt?nocache=1755386267 (line 19)) (1.10.0)\n",
            "Collecting evaluate>=0.4 (from -r https://raw.githubusercontent.com/semurillas/NLP_MIAA_252/refs/heads/main/requirements.txt?nocache=1755386267 (line 20))\n",
            "  Downloading evaluate-0.4.5-py3-none-any.whl.metadata (9.5 kB)\n",
            "Requirement already satisfied: pandas>=2.2 in /usr/local/lib/python3.11/dist-packages (from -r https://raw.githubusercontent.com/semurillas/NLP_MIAA_252/refs/heads/main/requirements.txt?nocache=1755386267 (line 23)) (2.2.2)\n",
            "Requirement already satisfied: matplotlib>=3.8 in /usr/local/lib/python3.11/dist-packages (from -r https://raw.githubusercontent.com/semurillas/NLP_MIAA_252/refs/heads/main/requirements.txt?nocache=1755386267 (line 24)) (3.10.0)\n",
            "Requirement already satisfied: seaborn>=0.12 in /usr/local/lib/python3.11/dist-packages (from -r https://raw.githubusercontent.com/semurillas/NLP_MIAA_252/refs/heads/main/requirements.txt?nocache=1755386267 (line 25)) (0.13.2)\n",
            "Requirement already satisfied: statsmodels>=0.14 in /usr/local/lib/python3.11/dist-packages (from -r https://raw.githubusercontent.com/semurillas/NLP_MIAA_252/refs/heads/main/requirements.txt?nocache=1755386267 (line 26)) (0.14.5)\n",
            "Requirement already satisfied: tqdm>=4.67 in /usr/local/lib/python3.11/dist-packages (from -r https://raw.githubusercontent.com/semurillas/NLP_MIAA_252/refs/heads/main/requirements.txt?nocache=1755386267 (line 27)) (4.67.1)\n",
            "Requirement already satisfied: bokeh>=3.5 in /usr/local/lib/python3.11/dist-packages (from -r https://raw.githubusercontent.com/semurillas/NLP_MIAA_252/refs/heads/main/requirements.txt?nocache=1755386267 (line 28)) (3.7.3)\n",
            "Requirement already satisfied: gradio>=4.36 in /usr/local/lib/python3.11/dist-packages (from -r https://raw.githubusercontent.com/semurillas/NLP_MIAA_252/refs/heads/main/requirements.txt?nocache=1755386267 (line 31)) (5.42.0)\n",
            "Collecting ollama>=0.2 (from -r https://raw.githubusercontent.com/semurillas/NLP_MIAA_252/refs/heads/main/requirements.txt?nocache=1755386267 (line 32))\n",
            "  Downloading ollama-0.5.3-py3-none-any.whl.metadata (4.3 kB)\n",
            "Requirement already satisfied: websockets>=14.0 in /usr/local/lib/python3.11/dist-packages (from -r https://raw.githubusercontent.com/semurillas/NLP_MIAA_252/refs/heads/main/requirements.txt?nocache=1755386267 (line 33)) (15.0.1)\n",
            "Collecting python-docx (from -r https://raw.githubusercontent.com/semurillas/NLP_MIAA_252/refs/heads/main/requirements.txt?nocache=1755386267 (line 34))\n",
            "  Downloading python_docx-1.2.0-py3-none-any.whl.metadata (2.0 kB)\n",
            "Requirement already satisfied: httpx<1.0.0,>=0.28.1 in /usr/local/lib/python3.11/dist-packages (from -r https://raw.githubusercontent.com/semurillas/NLP_MIAA_252/refs/heads/main/requirements.txt?nocache=1755386267 (line 35)) (0.28.1)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.11/dist-packages (from tensorboard==2.19.0->-r https://raw.githubusercontent.com/semurillas/NLP_MIAA_252/refs/heads/main/requirements.txt?nocache=1755386267 (line 18)) (1.4.0)\n",
            "Requirement already satisfied: grpcio>=1.48.2 in /usr/local/lib/python3.11/dist-packages (from tensorboard==2.19.0->-r https://raw.githubusercontent.com/semurillas/NLP_MIAA_252/refs/heads/main/requirements.txt?nocache=1755386267 (line 18)) (1.74.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.11/dist-packages (from tensorboard==2.19.0->-r https://raw.githubusercontent.com/semurillas/NLP_MIAA_252/refs/heads/main/requirements.txt?nocache=1755386267 (line 18)) (3.8.2)\n",
            "Requirement already satisfied: numpy>=1.12.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard==2.19.0->-r https://raw.githubusercontent.com/semurillas/NLP_MIAA_252/refs/heads/main/requirements.txt?nocache=1755386267 (line 18)) (2.0.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from tensorboard==2.19.0->-r https://raw.githubusercontent.com/semurillas/NLP_MIAA_252/refs/heads/main/requirements.txt?nocache=1755386267 (line 18)) (25.0)\n",
            "Requirement already satisfied: protobuf!=4.24.0,>=3.19.6 in /usr/local/lib/python3.11/dist-packages (from tensorboard==2.19.0->-r https://raw.githubusercontent.com/semurillas/NLP_MIAA_252/refs/heads/main/requirements.txt?nocache=1755386267 (line 18)) (5.29.5)\n",
            "Requirement already satisfied: six>1.9 in /usr/local/lib/python3.11/dist-packages (from tensorboard==2.19.0->-r https://raw.githubusercontent.com/semurillas/NLP_MIAA_252/refs/heads/main/requirements.txt?nocache=1755386267 (line 18)) (1.17.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard==2.19.0->-r https://raw.githubusercontent.com/semurillas/NLP_MIAA_252/refs/heads/main/requirements.txt?nocache=1755386267 (line 18)) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from tensorboard==2.19.0->-r https://raw.githubusercontent.com/semurillas/NLP_MIAA_252/refs/heads/main/requirements.txt?nocache=1755386267 (line 18)) (3.1.3)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.11/dist-packages (from spacy<3.9,>=3.8->-r https://raw.githubusercontent.com/semurillas/NLP_MIAA_252/refs/heads/main/requirements.txt?nocache=1755386267 (line 7)) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.9,>=3.8->-r https://raw.githubusercontent.com/semurillas/NLP_MIAA_252/refs/heads/main/requirements.txt?nocache=1755386267 (line 7)) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.9,>=3.8->-r https://raw.githubusercontent.com/semurillas/NLP_MIAA_252/refs/heads/main/requirements.txt?nocache=1755386267 (line 7)) (1.0.13)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.11/dist-packages (from spacy<3.9,>=3.8->-r https://raw.githubusercontent.com/semurillas/NLP_MIAA_252/refs/heads/main/requirements.txt?nocache=1755386267 (line 7)) (2.0.11)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.11/dist-packages (from spacy<3.9,>=3.8->-r https://raw.githubusercontent.com/semurillas/NLP_MIAA_252/refs/heads/main/requirements.txt?nocache=1755386267 (line 7)) (3.0.10)\n",
            "Requirement already satisfied: thinc<8.4.0,>=8.3.4 in /usr/local/lib/python3.11/dist-packages (from spacy<3.9,>=3.8->-r https://raw.githubusercontent.com/semurillas/NLP_MIAA_252/refs/heads/main/requirements.txt?nocache=1755386267 (line 7)) (8.3.6)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.11/dist-packages (from spacy<3.9,>=3.8->-r https://raw.githubusercontent.com/semurillas/NLP_MIAA_252/refs/heads/main/requirements.txt?nocache=1755386267 (line 7)) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.11/dist-packages (from spacy<3.9,>=3.8->-r https://raw.githubusercontent.com/semurillas/NLP_MIAA_252/refs/heads/main/requirements.txt?nocache=1755386267 (line 7)) (2.5.1)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.11/dist-packages (from spacy<3.9,>=3.8->-r https://raw.githubusercontent.com/semurillas/NLP_MIAA_252/refs/heads/main/requirements.txt?nocache=1755386267 (line 7)) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.9,>=3.8->-r https://raw.githubusercontent.com/semurillas/NLP_MIAA_252/refs/heads/main/requirements.txt?nocache=1755386267 (line 7)) (0.4.1)\n",
            "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.9,>=3.8->-r https://raw.githubusercontent.com/semurillas/NLP_MIAA_252/refs/heads/main/requirements.txt?nocache=1755386267 (line 7)) (0.16.0)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.9,>=3.8->-r https://raw.githubusercontent.com/semurillas/NLP_MIAA_252/refs/heads/main/requirements.txt?nocache=1755386267 (line 7)) (2.32.3)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.11/dist-packages (from spacy<3.9,>=3.8->-r https://raw.githubusercontent.com/semurillas/NLP_MIAA_252/refs/heads/main/requirements.txt?nocache=1755386267 (line 7)) (2.11.7)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from spacy<3.9,>=3.8->-r https://raw.githubusercontent.com/semurillas/NLP_MIAA_252/refs/heads/main/requirements.txt?nocache=1755386267 (line 7)) (3.1.6)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.9,>=3.8->-r https://raw.githubusercontent.com/semurillas/NLP_MIAA_252/refs/heads/main/requirements.txt?nocache=1755386267 (line 7)) (3.5.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk>=3.9->-r https://raw.githubusercontent.com/semurillas/NLP_MIAA_252/refs/heads/main/requirements.txt?nocache=1755386267 (line 8)) (8.2.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk>=3.9->-r https://raw.githubusercontent.com/semurillas/NLP_MIAA_252/refs/heads/main/requirements.txt?nocache=1755386267 (line 8)) (1.5.1)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk>=3.9->-r https://raw.githubusercontent.com/semurillas/NLP_MIAA_252/refs/heads/main/requirements.txt?nocache=1755386267 (line 8)) (2024.11.6)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers>=4.41->transformers[torch]>=4.41->-r https://raw.githubusercontent.com/semurillas/NLP_MIAA_252/refs/heads/main/requirements.txt?nocache=1755386267 (line 9)) (3.18.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.41->transformers[torch]>=4.41->-r https://raw.githubusercontent.com/semurillas/NLP_MIAA_252/refs/heads/main/requirements.txt?nocache=1755386267 (line 9)) (0.34.4)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.41->transformers[torch]>=4.41->-r https://raw.githubusercontent.com/semurillas/NLP_MIAA_252/refs/heads/main/requirements.txt?nocache=1755386267 (line 9)) (6.0.2)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.41->transformers[torch]>=4.41->-r https://raw.githubusercontent.com/semurillas/NLP_MIAA_252/refs/heads/main/requirements.txt?nocache=1755386267 (line 9)) (0.21.4)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.41->transformers[torch]>=4.41->-r https://raw.githubusercontent.com/semurillas/NLP_MIAA_252/refs/heads/main/requirements.txt?nocache=1755386267 (line 9)) (0.6.2)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.19->-r https://raw.githubusercontent.com/semurillas/NLP_MIAA_252/refs/heads/main/requirements.txt?nocache=1755386267 (line 10)) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.19->-r https://raw.githubusercontent.com/semurillas/NLP_MIAA_252/refs/heads/main/requirements.txt?nocache=1755386267 (line 10)) (0.3.8)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets>=2.19->-r https://raw.githubusercontent.com/semurillas/NLP_MIAA_252/refs/heads/main/requirements.txt?nocache=1755386267 (line 10)) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.19->-r https://raw.githubusercontent.com/semurillas/NLP_MIAA_252/refs/heads/main/requirements.txt?nocache=1755386267 (line 10)) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=2.19->-r https://raw.githubusercontent.com/semurillas/NLP_MIAA_252/refs/heads/main/requirements.txt?nocache=1755386267 (line 10)) (2025.3.0)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (from sentence-transformers>=3.0->-r https://raw.githubusercontent.com/semurillas/NLP_MIAA_252/refs/heads/main/requirements.txt?nocache=1755386267 (line 11)) (1.6.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from sentence-transformers>=3.0->-r https://raw.githubusercontent.com/semurillas/NLP_MIAA_252/refs/heads/main/requirements.txt?nocache=1755386267 (line 11)) (1.16.1)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.11/dist-packages (from sentence-transformers>=3.0->-r https://raw.githubusercontent.com/semurillas/NLP_MIAA_252/refs/heads/main/requirements.txt?nocache=1755386267 (line 11)) (11.3.0)\n",
            "Requirement already satisfied: typing_extensions>=4.5.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers>=3.0->-r https://raw.githubusercontent.com/semurillas/NLP_MIAA_252/refs/heads/main/requirements.txt?nocache=1755386267 (line 11)) (4.14.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=2.6->-r https://raw.githubusercontent.com/semurillas/NLP_MIAA_252/refs/heads/main/requirements.txt?nocache=1755386267 (line 14)) (3.5)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch>=2.6->-r https://raw.githubusercontent.com/semurillas/NLP_MIAA_252/refs/heads/main/requirements.txt?nocache=1755386267 (line 14))\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch>=2.6->-r https://raw.githubusercontent.com/semurillas/NLP_MIAA_252/refs/heads/main/requirements.txt?nocache=1755386267 (line 14))\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch>=2.6->-r https://raw.githubusercontent.com/semurillas/NLP_MIAA_252/refs/heads/main/requirements.txt?nocache=1755386267 (line 14))\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch>=2.6->-r https://raw.githubusercontent.com/semurillas/NLP_MIAA_252/refs/heads/main/requirements.txt?nocache=1755386267 (line 14))\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch>=2.6->-r https://raw.githubusercontent.com/semurillas/NLP_MIAA_252/refs/heads/main/requirements.txt?nocache=1755386267 (line 14))\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch>=2.6->-r https://raw.githubusercontent.com/semurillas/NLP_MIAA_252/refs/heads/main/requirements.txt?nocache=1755386267 (line 14))\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch>=2.6->-r https://raw.githubusercontent.com/semurillas/NLP_MIAA_252/refs/heads/main/requirements.txt?nocache=1755386267 (line 14))\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch>=2.6->-r https://raw.githubusercontent.com/semurillas/NLP_MIAA_252/refs/heads/main/requirements.txt?nocache=1755386267 (line 14))\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch>=2.6->-r https://raw.githubusercontent.com/semurillas/NLP_MIAA_252/refs/heads/main/requirements.txt?nocache=1755386267 (line 14))\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.6->-r https://raw.githubusercontent.com/semurillas/NLP_MIAA_252/refs/heads/main/requirements.txt?nocache=1755386267 (line 14)) (0.6.2)\n",
            "Collecting nvidia-nccl-cu12==2.21.5 (from torch>=2.6->-r https://raw.githubusercontent.com/semurillas/NLP_MIAA_252/refs/heads/main/requirements.txt?nocache=1755386267 (line 14))\n",
            "  Downloading nvidia_nccl_cu12-2.21.5-py3-none-manylinux2014_x86_64.whl.metadata (1.8 kB)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.6->-r https://raw.githubusercontent.com/semurillas/NLP_MIAA_252/refs/heads/main/requirements.txt?nocache=1755386267 (line 14)) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch>=2.6->-r https://raw.githubusercontent.com/semurillas/NLP_MIAA_252/refs/heads/main/requirements.txt?nocache=1755386267 (line 14))\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=2.6->-r https://raw.githubusercontent.com/semurillas/NLP_MIAA_252/refs/heads/main/requirements.txt?nocache=1755386267 (line 14)) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=2.6->-r https://raw.githubusercontent.com/semurillas/NLP_MIAA_252/refs/heads/main/requirements.txt?nocache=1755386267 (line 14)) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=2.6->-r https://raw.githubusercontent.com/semurillas/NLP_MIAA_252/refs/heads/main/requirements.txt?nocache=1755386267 (line 14)) (1.3.0)\n",
            "Collecting lightning-utilities<2.0,>=0.10.0 (from lightning>=2.2->-r https://raw.githubusercontent.com/semurillas/NLP_MIAA_252/refs/heads/main/requirements.txt?nocache=1755386267 (line 17))\n",
            "  Downloading lightning_utilities-0.15.2-py3-none-any.whl.metadata (5.7 kB)\n",
            "Collecting torchmetrics<3.0,>0.7.0 (from lightning>=2.2->-r https://raw.githubusercontent.com/semurillas/NLP_MIAA_252/refs/heads/main/requirements.txt?nocache=1755386267 (line 17))\n",
            "  Downloading torchmetrics-1.8.1-py3-none-any.whl.metadata (22 kB)\n",
            "Collecting pytorch-lightning (from lightning>=2.2->-r https://raw.githubusercontent.com/semurillas/NLP_MIAA_252/refs/heads/main/requirements.txt?nocache=1755386267 (line 17))\n",
            "  Downloading pytorch_lightning-2.5.3-py3-none-any.whl.metadata (20 kB)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from accelerate>=0.30->-r https://raw.githubusercontent.com/semurillas/NLP_MIAA_252/refs/heads/main/requirements.txt?nocache=1755386267 (line 19)) (5.9.5)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas>=2.2->-r https://raw.githubusercontent.com/semurillas/NLP_MIAA_252/refs/heads/main/requirements.txt?nocache=1755386267 (line 23)) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=2.2->-r https://raw.githubusercontent.com/semurillas/NLP_MIAA_252/refs/heads/main/requirements.txt?nocache=1755386267 (line 23)) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=2.2->-r https://raw.githubusercontent.com/semurillas/NLP_MIAA_252/refs/heads/main/requirements.txt?nocache=1755386267 (line 23)) (2025.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.8->-r https://raw.githubusercontent.com/semurillas/NLP_MIAA_252/refs/heads/main/requirements.txt?nocache=1755386267 (line 24)) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.8->-r https://raw.githubusercontent.com/semurillas/NLP_MIAA_252/refs/heads/main/requirements.txt?nocache=1755386267 (line 24)) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.8->-r https://raw.githubusercontent.com/semurillas/NLP_MIAA_252/refs/heads/main/requirements.txt?nocache=1755386267 (line 24)) (4.59.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.8->-r https://raw.githubusercontent.com/semurillas/NLP_MIAA_252/refs/heads/main/requirements.txt?nocache=1755386267 (line 24)) (1.4.9)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.8->-r https://raw.githubusercontent.com/semurillas/NLP_MIAA_252/refs/heads/main/requirements.txt?nocache=1755386267 (line 24)) (3.2.3)\n",
            "Requirement already satisfied: patsy>=0.5.6 in /usr/local/lib/python3.11/dist-packages (from statsmodels>=0.14->-r https://raw.githubusercontent.com/semurillas/NLP_MIAA_252/refs/heads/main/requirements.txt?nocache=1755386267 (line 26)) (1.0.1)\n",
            "Requirement already satisfied: narwhals>=1.13 in /usr/local/lib/python3.11/dist-packages (from bokeh>=3.5->-r https://raw.githubusercontent.com/semurillas/NLP_MIAA_252/refs/heads/main/requirements.txt?nocache=1755386267 (line 28)) (2.1.1)\n",
            "Requirement already satisfied: tornado>=6.2 in /usr/local/lib/python3.11/dist-packages (from bokeh>=3.5->-r https://raw.githubusercontent.com/semurillas/NLP_MIAA_252/refs/heads/main/requirements.txt?nocache=1755386267 (line 28)) (6.4.2)\n",
            "Requirement already satisfied: xyzservices>=2021.09.1 in /usr/local/lib/python3.11/dist-packages (from bokeh>=3.5->-r https://raw.githubusercontent.com/semurillas/NLP_MIAA_252/refs/heads/main/requirements.txt?nocache=1755386267 (line 28)) (2025.4.0)\n",
            "Requirement already satisfied: aiofiles<25.0,>=22.0 in /usr/local/lib/python3.11/dist-packages (from gradio>=4.36->-r https://raw.githubusercontent.com/semurillas/NLP_MIAA_252/refs/heads/main/requirements.txt?nocache=1755386267 (line 31)) (24.1.0)\n",
            "Requirement already satisfied: anyio<5.0,>=3.0 in /usr/local/lib/python3.11/dist-packages (from gradio>=4.36->-r https://raw.githubusercontent.com/semurillas/NLP_MIAA_252/refs/heads/main/requirements.txt?nocache=1755386267 (line 31)) (4.10.0)\n",
            "Requirement already satisfied: brotli>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from gradio>=4.36->-r https://raw.githubusercontent.com/semurillas/NLP_MIAA_252/refs/heads/main/requirements.txt?nocache=1755386267 (line 31)) (1.1.0)\n",
            "Requirement already satisfied: fastapi<1.0,>=0.115.2 in /usr/local/lib/python3.11/dist-packages (from gradio>=4.36->-r https://raw.githubusercontent.com/semurillas/NLP_MIAA_252/refs/heads/main/requirements.txt?nocache=1755386267 (line 31)) (0.116.1)\n",
            "Requirement already satisfied: ffmpy in /usr/local/lib/python3.11/dist-packages (from gradio>=4.36->-r https://raw.githubusercontent.com/semurillas/NLP_MIAA_252/refs/heads/main/requirements.txt?nocache=1755386267 (line 31)) (0.6.1)\n",
            "Requirement already satisfied: gradio-client==1.11.1 in /usr/local/lib/python3.11/dist-packages (from gradio>=4.36->-r https://raw.githubusercontent.com/semurillas/NLP_MIAA_252/refs/heads/main/requirements.txt?nocache=1755386267 (line 31)) (1.11.1)\n",
            "Requirement already satisfied: groovy~=0.1 in /usr/local/lib/python3.11/dist-packages (from gradio>=4.36->-r https://raw.githubusercontent.com/semurillas/NLP_MIAA_252/refs/heads/main/requirements.txt?nocache=1755386267 (line 31)) (0.1.2)\n",
            "Requirement already satisfied: markupsafe<4.0,>=2.0 in /usr/local/lib/python3.11/dist-packages (from gradio>=4.36->-r https://raw.githubusercontent.com/semurillas/NLP_MIAA_252/refs/heads/main/requirements.txt?nocache=1755386267 (line 31)) (3.0.2)\n",
            "Requirement already satisfied: orjson~=3.0 in /usr/local/lib/python3.11/dist-packages (from gradio>=4.36->-r https://raw.githubusercontent.com/semurillas/NLP_MIAA_252/refs/heads/main/requirements.txt?nocache=1755386267 (line 31)) (3.11.2)\n",
            "Requirement already satisfied: pydub in /usr/local/lib/python3.11/dist-packages (from gradio>=4.36->-r https://raw.githubusercontent.com/semurillas/NLP_MIAA_252/refs/heads/main/requirements.txt?nocache=1755386267 (line 31)) (0.25.1)\n",
            "Requirement already satisfied: python-multipart>=0.0.18 in /usr/local/lib/python3.11/dist-packages (from gradio>=4.36->-r https://raw.githubusercontent.com/semurillas/NLP_MIAA_252/refs/heads/main/requirements.txt?nocache=1755386267 (line 31)) (0.0.20)\n",
            "Requirement already satisfied: ruff>=0.9.3 in /usr/local/lib/python3.11/dist-packages (from gradio>=4.36->-r https://raw.githubusercontent.com/semurillas/NLP_MIAA_252/refs/heads/main/requirements.txt?nocache=1755386267 (line 31)) (0.12.8)\n",
            "Requirement already satisfied: safehttpx<0.2.0,>=0.1.6 in /usr/local/lib/python3.11/dist-packages (from gradio>=4.36->-r https://raw.githubusercontent.com/semurillas/NLP_MIAA_252/refs/heads/main/requirements.txt?nocache=1755386267 (line 31)) (0.1.6)\n",
            "Requirement already satisfied: semantic-version~=2.0 in /usr/local/lib/python3.11/dist-packages (from gradio>=4.36->-r https://raw.githubusercontent.com/semurillas/NLP_MIAA_252/refs/heads/main/requirements.txt?nocache=1755386267 (line 31)) (2.10.0)\n",
            "Requirement already satisfied: starlette<1.0,>=0.40.0 in /usr/local/lib/python3.11/dist-packages (from gradio>=4.36->-r https://raw.githubusercontent.com/semurillas/NLP_MIAA_252/refs/heads/main/requirements.txt?nocache=1755386267 (line 31)) (0.47.2)\n",
            "Requirement already satisfied: tomlkit<0.14.0,>=0.12.0 in /usr/local/lib/python3.11/dist-packages (from gradio>=4.36->-r https://raw.githubusercontent.com/semurillas/NLP_MIAA_252/refs/heads/main/requirements.txt?nocache=1755386267 (line 31)) (0.13.3)\n",
            "Requirement already satisfied: uvicorn>=0.14.0 in /usr/local/lib/python3.11/dist-packages (from gradio>=4.36->-r https://raw.githubusercontent.com/semurillas/NLP_MIAA_252/refs/heads/main/requirements.txt?nocache=1755386267 (line 31)) (0.35.0)\n",
            "Requirement already satisfied: lxml>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from python-docx->-r https://raw.githubusercontent.com/semurillas/NLP_MIAA_252/refs/heads/main/requirements.txt?nocache=1755386267 (line 34)) (5.4.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx<1.0.0,>=0.28.1->-r https://raw.githubusercontent.com/semurillas/NLP_MIAA_252/refs/heads/main/requirements.txt?nocache=1755386267 (line 35)) (2025.8.3)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1.0.0,>=0.28.1->-r https://raw.githubusercontent.com/semurillas/NLP_MIAA_252/refs/heads/main/requirements.txt?nocache=1755386267 (line 35)) (1.0.9)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from httpx<1.0.0,>=0.28.1->-r https://raw.githubusercontent.com/semurillas/NLP_MIAA_252/refs/heads/main/requirements.txt?nocache=1755386267 (line 35)) (3.10)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1.0.0,>=0.28.1->-r https://raw.githubusercontent.com/semurillas/NLP_MIAA_252/refs/heads/main/requirements.txt?nocache=1755386267 (line 35)) (0.16.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio<5.0,>=3.0->gradio>=4.36->-r https://raw.githubusercontent.com/semurillas/NLP_MIAA_252/refs/heads/main/requirements.txt?nocache=1755386267 (line 31)) (1.3.1)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=2.19->-r https://raw.githubusercontent.com/semurillas/NLP_MIAA_252/refs/heads/main/requirements.txt?nocache=1755386267 (line 10)) (3.12.15)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers>=4.41->transformers[torch]>=4.41->-r https://raw.githubusercontent.com/semurillas/NLP_MIAA_252/refs/heads/main/requirements.txt?nocache=1755386267 (line 9)) (1.1.7)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.11/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy<3.9,>=3.8->-r https://raw.githubusercontent.com/semurillas/NLP_MIAA_252/refs/heads/main/requirements.txt?nocache=1755386267 (line 7)) (1.3.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.9,>=3.8->-r https://raw.githubusercontent.com/semurillas/NLP_MIAA_252/refs/heads/main/requirements.txt?nocache=1755386267 (line 7)) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.9,>=3.8->-r https://raw.githubusercontent.com/semurillas/NLP_MIAA_252/refs/heads/main/requirements.txt?nocache=1755386267 (line 7)) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.9,>=3.8->-r https://raw.githubusercontent.com/semurillas/NLP_MIAA_252/refs/heads/main/requirements.txt?nocache=1755386267 (line 7)) (0.4.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.9,>=3.8->-r https://raw.githubusercontent.com/semurillas/NLP_MIAA_252/refs/heads/main/requirements.txt?nocache=1755386267 (line 7)) (3.4.3)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.9,>=3.8->-r https://raw.githubusercontent.com/semurillas/NLP_MIAA_252/refs/heads/main/requirements.txt?nocache=1755386267 (line 7)) (2.5.0)\n",
            "Requirement already satisfied: blis<1.4.0,>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from thinc<8.4.0,>=8.3.4->spacy<3.9,>=3.8->-r https://raw.githubusercontent.com/semurillas/NLP_MIAA_252/refs/heads/main/requirements.txt?nocache=1755386267 (line 7)) (1.3.0)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.11/dist-packages (from thinc<8.4.0,>=8.3.4->spacy<3.9,>=3.8->-r https://raw.githubusercontent.com/semurillas/NLP_MIAA_252/refs/heads/main/requirements.txt?nocache=1755386267 (line 7)) (0.1.5)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.9,>=3.8->-r https://raw.githubusercontent.com/semurillas/NLP_MIAA_252/refs/heads/main/requirements.txt?nocache=1755386267 (line 7)) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.9,>=3.8->-r https://raw.githubusercontent.com/semurillas/NLP_MIAA_252/refs/heads/main/requirements.txt?nocache=1755386267 (line 7)) (13.9.4)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from weasel<0.5.0,>=0.1.0->spacy<3.9,>=3.8->-r https://raw.githubusercontent.com/semurillas/NLP_MIAA_252/refs/heads/main/requirements.txt?nocache=1755386267 (line 7)) (0.21.1)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.11/dist-packages (from weasel<0.5.0,>=0.1.0->spacy<3.9,>=3.8->-r https://raw.githubusercontent.com/semurillas/NLP_MIAA_252/refs/heads/main/requirements.txt?nocache=1755386267 (line 7)) (7.3.0.post1)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->sentence-transformers>=3.0->-r https://raw.githubusercontent.com/semurillas/NLP_MIAA_252/refs/heads/main/requirements.txt?nocache=1755386267 (line 11)) (3.6.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=2.19->-r https://raw.githubusercontent.com/semurillas/NLP_MIAA_252/refs/heads/main/requirements.txt?nocache=1755386267 (line 10)) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=2.19->-r https://raw.githubusercontent.com/semurillas/NLP_MIAA_252/refs/heads/main/requirements.txt?nocache=1755386267 (line 10)) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=2.19->-r https://raw.githubusercontent.com/semurillas/NLP_MIAA_252/refs/heads/main/requirements.txt?nocache=1755386267 (line 10)) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=2.19->-r https://raw.githubusercontent.com/semurillas/NLP_MIAA_252/refs/heads/main/requirements.txt?nocache=1755386267 (line 10)) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=2.19->-r https://raw.githubusercontent.com/semurillas/NLP_MIAA_252/refs/heads/main/requirements.txt?nocache=1755386267 (line 10)) (6.6.4)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=2.19->-r https://raw.githubusercontent.com/semurillas/NLP_MIAA_252/refs/heads/main/requirements.txt?nocache=1755386267 (line 10)) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=2.19->-r https://raw.githubusercontent.com/semurillas/NLP_MIAA_252/refs/heads/main/requirements.txt?nocache=1755386267 (line 10)) (1.20.1)\n",
            "Requirement already satisfied: marisa-trie>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy<3.9,>=3.8->-r https://raw.githubusercontent.com/semurillas/NLP_MIAA_252/refs/heads/main/requirements.txt?nocache=1755386267 (line 7)) (1.2.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.9,>=3.8->-r https://raw.githubusercontent.com/semurillas/NLP_MIAA_252/refs/heads/main/requirements.txt?nocache=1755386267 (line 7)) (4.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.9,>=3.8->-r https://raw.githubusercontent.com/semurillas/NLP_MIAA_252/refs/heads/main/requirements.txt?nocache=1755386267 (line 7)) (2.19.2)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy<3.9,>=3.8->-r https://raw.githubusercontent.com/semurillas/NLP_MIAA_252/refs/heads/main/requirements.txt?nocache=1755386267 (line 7)) (1.17.3)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.9,>=3.8->-r https://raw.githubusercontent.com/semurillas/NLP_MIAA_252/refs/heads/main/requirements.txt?nocache=1755386267 (line 7)) (0.1.2)\n",
            "Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m76.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m60.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m40.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m753.1 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m11.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nccl_cu12-2.21.5-py3-none-manylinux2014_x86_64.whl (188.7 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m188.7/188.7 MB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m63.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading lightning-2.5.3-py3-none-any.whl (824 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m824.2/824.2 kB\u001b[0m \u001b[31m26.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading evaluate-0.4.5-py3-none-any.whl (84 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m84.1/84.1 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ollama-0.5.3-py3-none-any.whl (13 kB)\n",
            "Downloading python_docx-1.2.0-py3-none-any.whl (252 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m253.0/253.0 kB\u001b[0m \u001b[31m13.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading lightning_utilities-0.15.2-py3-none-any.whl (29 kB)\n",
            "Downloading torchmetrics-1.8.1-py3-none-any.whl (982 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m983.0/983.0 kB\u001b[0m \u001b[31m23.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pytorch_lightning-2.5.3-py3-none-any.whl (828 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m828.2/828.2 kB\u001b[0m \u001b[31m22.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: python-docx, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, lightning-utilities, nvidia-cusparse-cu12, nvidia-cudnn-cu12, ollama, nvidia-cusolver-cu12, torchmetrics, evaluate, pytorch-lightning, lightning\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-nccl-cu12\n",
            "    Found existing installation: nvidia-nccl-cu12 2.23.4\n",
            "    Uninstalling nvidia-nccl-cu12-2.23.4:\n",
            "      Successfully uninstalled nvidia-nccl-cu12-2.23.4\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed evaluate-0.4.5 lightning-2.5.3 lightning-utilities-0.15.2 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nccl-cu12-2.21.5 nvidia-nvjitlink-cu12-12.4.127 ollama-0.5.3 python-docx-1.2.0 pytorch-lightning-2.5.3 torchmetrics-1.8.1\n"
          ]
        }
      ],
      "source": [
        "!test '{IN_COLAB}' = 'True' && pip install -r \"https://raw.githubusercontent.com/semurillas/NLP_MIAA_252/refs/heads/main/requirements.txt?nocache=$(date +%s)\""
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Descarga del Pipeline de Idioma a usar Con SPACY\n",
        "\n",
        "Decidimos usar el Pipeline entrenado de Idioma EspaÃ±ol: **es_dep_news_trf** (https://spacy.io/models/es)\n",
        "\n",
        "El cÃ³digo a continuaciÃ³n lo descarga para su uso con **SPACY**"
      ],
      "metadata": {
        "id": "XnJWKts_mwHH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m spacy download es_dep_news_trf"
      ],
      "metadata": {
        "id": "vQpXXAHDlROI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ec587424-f696-40be-989e-1e033f622533"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting es-dep-news-trf==3.8.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/es_dep_news_trf-3.8.0/es_dep_news_trf-3.8.0-py3-none-any.whl (407.8 MB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m407.8/407.8 MB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting spacy-curated-transformers<1.0.0,>=0.2.2 (from es-dep-news-trf==3.8.0)\n",
            "  Downloading spacy_curated_transformers-0.3.1-py2.py3-none-any.whl.metadata (2.7 kB)\n",
            "Collecting curated-transformers<0.2.0,>=0.1.0 (from spacy-curated-transformers<1.0.0,>=0.2.2->es-dep-news-trf==3.8.0)\n",
            "  Downloading curated_transformers-0.1.1-py2.py3-none-any.whl.metadata (965 bytes)\n",
            "Collecting curated-tokenizers<0.1.0,>=0.0.9 (from spacy-curated-transformers<1.0.0,>=0.2.2->es-dep-news-trf==3.8.0)\n",
            "  Downloading curated_tokenizers-0.0.9-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.9 kB)\n",
            "Requirement already satisfied: torch>=1.12.0 in /usr/local/lib/python3.11/dist-packages (from spacy-curated-transformers<1.0.0,>=0.2.2->es-dep-news-trf==3.8.0) (2.6.0+cu124)\n",
            "Requirement already satisfied: regex>=2022 in /usr/local/lib/python3.11/dist-packages (from curated-tokenizers<0.1.0,>=0.0.9->spacy-curated-transformers<1.0.0,>=0.2.2->es-dep-news-trf==3.8.0) (2024.11.6)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch>=1.12.0->spacy-curated-transformers<1.0.0,>=0.2.2->es-dep-news-trf==3.8.0) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.12.0->spacy-curated-transformers<1.0.0,>=0.2.2->es-dep-news-trf==3.8.0) (4.14.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.12.0->spacy-curated-transformers<1.0.0,>=0.2.2->es-dep-news-trf==3.8.0) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.12.0->spacy-curated-transformers<1.0.0,>=0.2.2->es-dep-news-trf==3.8.0) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch>=1.12.0->spacy-curated-transformers<1.0.0,>=0.2.2->es-dep-news-trf==3.8.0) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.12.0->spacy-curated-transformers<1.0.0,>=0.2.2->es-dep-news-trf==3.8.0) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.12.0->spacy-curated-transformers<1.0.0,>=0.2.2->es-dep-news-trf==3.8.0) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.12.0->spacy-curated-transformers<1.0.0,>=0.2.2->es-dep-news-trf==3.8.0) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=1.12.0->spacy-curated-transformers<1.0.0,>=0.2.2->es-dep-news-trf==3.8.0) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=1.12.0->spacy-curated-transformers<1.0.0,>=0.2.2->es-dep-news-trf==3.8.0) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=1.12.0->spacy-curated-transformers<1.0.0,>=0.2.2->es-dep-news-trf==3.8.0) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=1.12.0->spacy-curated-transformers<1.0.0,>=0.2.2->es-dep-news-trf==3.8.0) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=1.12.0->spacy-curated-transformers<1.0.0,>=0.2.2->es-dep-news-trf==3.8.0) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=1.12.0->spacy-curated-transformers<1.0.0,>=0.2.2->es-dep-news-trf==3.8.0) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.12.0->spacy-curated-transformers<1.0.0,>=0.2.2->es-dep-news-trf==3.8.0) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.12.0->spacy-curated-transformers<1.0.0,>=0.2.2->es-dep-news-trf==3.8.0) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.12.0->spacy-curated-transformers<1.0.0,>=0.2.2->es-dep-news-trf==3.8.0) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.12.0->spacy-curated-transformers<1.0.0,>=0.2.2->es-dep-news-trf==3.8.0) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.12.0->spacy-curated-transformers<1.0.0,>=0.2.2->es-dep-news-trf==3.8.0) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.12.0->spacy-curated-transformers<1.0.0,>=0.2.2->es-dep-news-trf==3.8.0) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.12.0->spacy-curated-transformers<1.0.0,>=0.2.2->es-dep-news-trf==3.8.0) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.12.0->spacy-curated-transformers<1.0.0,>=0.2.2->es-dep-news-trf==3.8.0) (3.0.2)\n",
            "Downloading spacy_curated_transformers-0.3.1-py2.py3-none-any.whl (237 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m237.9/237.9 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading curated_tokenizers-0.0.9-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (735 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m735.6/735.6 kB\u001b[0m \u001b[31m20.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading curated_transformers-0.1.1-py2.py3-none-any.whl (25 kB)\n",
            "Installing collected packages: curated-tokenizers, curated-transformers, spacy-curated-transformers, es-dep-news-trf\n",
            "Successfully installed curated-tokenizers-0.0.9 curated-transformers-0.1.1 es-dep-news-trf-3.8.0 spacy-curated-transformers-0.3.1\n",
            "\u001b[38;5;2mâœ” Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('es_dep_news_trf')\n",
            "\u001b[38;5;3mâš  Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Nota 1**\n",
        "\n",
        "Por favor, recuerde realizar\n",
        "\n",
        "**MenÃº superior --> Runtime --> Restart runtime**\n",
        "\n",
        "Antes de continuar con los siguientes pasos."
      ],
      "metadata": {
        "id": "pwmsydGvo-lf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Cargue del Pipeline Seleccionado a una Instancia de NLP\n",
        "\n",
        "Con la descarga del Pipeline entrenado del idioma EspaÃ±ol **es_dep_news_trf** que se seleccionÃ³ lo \"cargamos\" a una instancia NLP de la librerÃ­a **SPACY**, para poder hacer procesamiento de algÃºn texto en idioma **EspaÃ±ol**"
      ],
      "metadata": {
        "id": "4l9Dzod4nY3U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "nlp = spacy.load('es_dep_news_trf')"
      ],
      "metadata": {
        "id": "V99A4u3mZK8R"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5. AnÃ¡lisis de Texto con Instancia de SPACY y Pipeline es_dep_news_trf\n",
        "\n",
        "Vamos a usar **SPACY** con el Pipeline Entrenado que hemos seleccionado **es_dep_news_trf** (https://spacy.io/models/es)\n",
        "\n",
        "Seleccionamos un texto correspondiente a un documento **Word** denominado *Proyecto II de InnovaciÃ³n TecnolÃ³gica*.\n",
        "\n",
        "Dado que el archivo estÃ¡ en formato Word (**.docx**), debemos primero convertirlo a un texto antes de su procesamiento por **SPACY**"
      ],
      "metadata": {
        "id": "fP-JOPvoplMA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import docx           # Libreria requerida para trabajar con documentos Word\n",
        "import requests       # Libreria requerida para hacer HTTP requests\n",
        "\n",
        "# Paso 1: Descargar el documento del Proyecto de GitHub en la ruta especificada.\n",
        "url = \"https://raw.githubusercontent.com/semurillas/NLP_MIAA_252/main/Texts/Proyecto%20-%20Despliegue%20Modelos%20IA%20-%20Microservicios%20-%20Informe%20Final%20v3.docx\"\n",
        "docx_file_path = \"informe_final.docx\"\n",
        "\n",
        "response = requests.get(url)\n",
        "with open(docx_file_path, \"wb\") as f:\n",
        "    f.write(response.content)\n",
        "\n",
        "print(f\"Archivo descargado como: {docx_file_path} \\n\")     # ConfirmaciÃ³n de que el archivo seleccionado haya sido descargado exitosamente.\n",
        "\n",
        "# Paso 2: FunciÃ³n para leer el archivo Word descargado y llevar su contenido a formato texto\n",
        "def read_docx(file_path):\n",
        "    \"\"\"Reads a .docx file and returns its text content.\"\"\"\n",
        "    doc = docx.Document(file_path)\n",
        "    all_text = []\n",
        "    for paragraph in doc.paragraphs:\n",
        "        all_text.append(paragraph.text)\n",
        "    return '\\n'.join(all_text)\n",
        "\n",
        "# Paso 3: EjecuciÃ³n de la FunciÃ³n read_docx() para transformar el contenido del archivo Word a Texto\n",
        "text_content = read_docx(docx_file_path)\n",
        "\n",
        "# Paso 4: Procesamiento del archivo texto obtenido con la instancia NLP basada en \"SPACY\"\n",
        "doc = nlp(text_content)\n",
        "print(docx_file_path,' has been processed with NLP Spacy model succesfully \\n')\n"
      ],
      "metadata": {
        "id": "4eCDCBQ9m9CK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "021331af-671f-4d9f-e7e4-cdf74e8bcc1e"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archivo descargado como: informe_final.docx \n",
            "\n",
            "informe_final.docx  has been processed with NLP model succesfully \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6. AnÃ¡lisis del Documento procesado con SPACY y Lenguaje EspaÃ±ol\n",
        "\n",
        "Usando el documento procesado por NLP basado en **SPACY**, vamos a obtener:\n",
        "- Cantidad de Tokens en el documento analizado\n",
        "- Cantidad de Tokens Ãšnicos en el documento Analizado\n",
        "- Muestra de una cantidad de Tokens Ãšnicos\n",
        "- Contar y Mostrar los Tokens mas usados en el documento analizado\n",
        "- Oraciones encontradas en el documento analizado\n",
        "- Mostrar (Imprimir) alguna OraciÃ³n seleccionada del documento analizado\n",
        "- Mostrar Text, POS, dep y Lemma de los Tokens en la OraciÃ³n seleccionada\n",
        "- Uso de Matcher con un PatrÃ³n definido y encontrar cantidad de ocurrencias en el documento analizado.\n",
        "- Mostrar palabras previas y posteriores a las Ocurrencias Encontradas en el documento analizado\n",
        "- Mostrar las Oraciones que contienen las Ocurrencias Encontradas en el documento analizado."
      ],
      "metadata": {
        "id": "zmxmleu4qiuk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 6.1. Cantidad de Tokens en el Documento Analizado"
      ],
      "metadata": {
        "id": "RR4ssxtlroa7"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "PC6KlqG-bPPT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "52dcefed-adde-49d9-8a90-6bf6fb82cfaf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "El documento analizado contiene:  4932 Tokens\n"
          ]
        }
      ],
      "source": [
        "print(\"El documento analizado contiene: \", len(doc), \"Tokens\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 6.2. Cantidad de Tokens Ãšnicos en el Documento Analizado"
      ],
      "metadata": {
        "id": "NH11B0hlsHXP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "unique_tokens = set([token.text for token in doc])\n",
        "print(f\"El documento analizado contiene :\", len(unique_tokens), \"Tokens Ãºnicos\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gXfijkC_edip",
        "outputId": "9e4f4c55-75e5-42a7-bf19-a673e61bd4dd"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "El documento analizado contiene : 1218 Tokens Ãºnicos\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 6.3. Muestra de Tokens Ãšnicos en el Documento Analizado"
      ],
      "metadata": {
        "id": "t8Zq8oossc9V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ImmpresiÃ³n de Encabezado en Pantalla\n",
        "print(\"{:20}{:20}{:20}{:20}\".format(\"Text\", \"POS\", \"dep\", \"lemma\"))\n",
        "print(\"-----------------------------------------------------------------\")\n",
        "\n",
        "# SelecciÃ³n de Cantidad de Tokens Ãšnicos a Mostrar\n",
        "cant_unique_tokens = 20\n",
        "\n",
        "# Ciclo para imprimir en Pantalla la cantidad de Tokens Ãšnicos seleccionados\n",
        "for token in list(unique_tokens)[:cant_unique_tokens]:\n",
        "    # Encuentra la primera ocurrencia del Token Ãšnico en el documento\n",
        "    for t in doc:\n",
        "        if t.text == token:\n",
        "            print(f\"{t.text:{20}}{t.pos_:{20}}{t.dep_:{20}}{t.lemma_:{20}}\")\n",
        "            break # Nos movemos al siguiente Token Ãšnico"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vgIHvmAweuZa",
        "outputId": "22b0e164-d9d7-45e2-a6b3-27525b99fea8"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Text                POS                 dep                 lemma               \n",
            "-----------------------------------------------------------------\n",
            "enfoque             NOUN                obl                 enfoque             \n",
            "Biomedical          PROPN               flat                Biomedical          \n",
            "frameworks          NOUN                nsubj               framework           \n",
            "open                PROPN               flat                open                \n",
            "modernas            ADJ                 amod                moderno             \n",
            "ApÃ©ndice            PROPN               flat                ApÃ©ndice            \n",
            "etapas              NOUN                obl                 etapa               \n",
            "robusta             ADJ                 amod                robusto             \n",
            "referencia          NOUN                nmod                referencia          \n",
            "Bajo                ADP                 case                bajo                \n",
            "basadas             ADJ                 amod                basado              \n",
            "programaciÃ³n        NOUN                nmod                programaciÃ³n        \n",
            "usando              VERB                advcl               usar                \n",
            "escenarios          NOUN                nmod                escenario           \n",
            "Damiani             PROPN               flat                Damiani             \n",
            "estandarizada       ADJ                 amod                estandarizado       \n",
            "OpenShift           PROPN               conj                OpenShift           \n",
            "soluciones          NOUN                nmod                soluciÃ³n            \n",
            "opcional            ADJ                 amod                opcional            \n",
            "Su                  DET                 det                 su                  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 6.4. Contar y Mostrar los Tokens mÃ¡s Usados en el Documento Analizado"
      ],
      "metadata": {
        "id": "e2kL7fcvuCL2"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bd192d7f",
        "outputId": "6cac0c57-1995-4e5a-a2c5-fca01409c432"
      },
      "source": [
        "from collections import Counter\n",
        "\n",
        "# Removemos puntuaciÃ³n y palabras \"stop\" para el analisis de frecuencia de Tokens\n",
        "filtered_tokens = [token.text for token in doc if not token.is_punct and not token.is_stop]\n",
        "\n",
        "# Conteo de la frecuencia de cada Token\n",
        "token_counts = Counter(filtered_tokens)\n",
        "\n",
        "# Mostrar en Pantalla los 20 Tokens mas encontrados (Mayor a Menor)\n",
        "print(\"Tokens mas encontrados (excluyendo puntuaciÃ³n y palabras stop):\\n\")\n",
        "print(\"{:20}{:20}{:20}\".format(\"Token\", \"||\", \"Cantidad\"))\n",
        "print(\"-------------------------------------------------\")\n",
        "\n",
        "for token, count in token_counts.most_common(20):\n",
        "    print(\"{:20}{:20}{:10}\".format(token, \"||\", count))"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokens mas encontrados (excluyendo puntuaciÃ³n y palabras stop):\n",
            "\n",
            "Token               ||                  Cantidad            \n",
            "-------------------------------------------------\n",
            "\n",
            "                   ||                         129\n",
            "microservicios      ||                          42\n",
            "despliegue          ||                          26\n",
            "\n",
            "\n",
            "                  ||                          25\n",
            "Docker              ||                          23\n",
            "modelos             ||                          22\n",
            "desarrollo          ||                          20\n",
            "IA                  ||                          19\n",
            "and                 ||                          17\n",
            "plataforma          ||                          16\n",
            "contenedores        ||                          16\n",
            "entornos            ||                          15\n",
            "sistema             ||                          14\n",
            "microservicio       ||                          13\n",
            "servicios           ||                          13\n",
            "Microservices       ||                          13\n",
            "API                 ||                          12\n",
            "enfoque             ||                          12\n",
            "CI                  ||                          12\n",
            "CD                  ||                          12\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 6.5. Conteo de Oraciones en el Documento Analizado"
      ],
      "metadata": {
        "id": "QGrQVnqDv_5d"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "UObgeX6FbPPT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bc2c869b-833d-437b-b5f4-b7c4ed2dfa37"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "El documento analizado tiene:  214 oraciones\n"
          ]
        }
      ],
      "source": [
        "oraciones = list(doc.sents)\n",
        "print(\"El documento analizado tiene: \", len(oraciones), \"oraciones\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 6.6. Imprimir una de las Oraciones en el Documento Analizado"
      ],
      "metadata": {
        "id": "yDBhO4-LwUVV"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "Hh7xnToDbPPT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5780c585-c166-44ad-f7fb-33dea9db95fb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "La oraciÃ³n Nro. 3 es: \n",
            " La soluciÃ³n fue diseÃ±ada en respuesta a la propuesta y necesidades del cliente, considerando sus limitaciones actuales en infraestructura y software, y orientada a ofrecer escalabilidad, flexibilidad y facilidad de mantenimiento.\n"
          ]
        }
      ],
      "source": [
        "# Numero de OraciÃ³n a imprimir\n",
        "numero_oracion = 3\n",
        "\n",
        "# Chequeo de que el NÃºmero de OraciÃ³n este en el rango de la cantidad encontrada\n",
        "# en el documento analizado\n",
        "if numero_oracion > 0 & numero_oracion <= len(oraciones):\n",
        "    print(\"La oraciÃ³n Nro.\", numero_oracion, \"es: \\n\", oraciones[numero_oracion-1])\n",
        "else:\n",
        "    print(f\"No hay una oraciÃ³n con el nÃºmero {numero_oracion} en el documento analizado\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 6.7. Imprimir Token, text, POS tag, dep tag y lemma de la OraciÃ³n seleccionada en el paso 6.6."
      ],
      "metadata": {
        "id": "B0600kOixmr-"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "fscVIMCYbPPU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8e038880-8bb8-47ed-c6f8-ba5cce96fa6b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Text                POS                 dep                 lemma               \n",
            "-----------------------------------------------------------------\n",
            "La                  DET                 det                 el                  \n",
            "soluciÃ³n            NOUN                nsubj               soluciÃ³n            \n",
            "fue                 AUX                 aux                 ser                 \n",
            "diseÃ±ada            VERB                ROOT                diseÃ±ar             \n",
            "en                  ADP                 case                en                  \n",
            "respuesta           NOUN                obl                 respuesta           \n",
            "a                   ADP                 case                a                   \n",
            "la                  DET                 det                 el                  \n",
            "propuesta           NOUN                nmod                propuesta           \n",
            "y                   CCONJ               cc                  y                   \n",
            "necesidades         NOUN                conj                necesidad           \n",
            "del                 ADP                 case                del                 \n",
            "cliente             NOUN                nmod                cliente             \n",
            ",                   PUNCT               punct               ,                   \n",
            "considerando        VERB                advcl               considerar          \n",
            "sus                 DET                 det                 su                  \n",
            "limitaciones        NOUN                obj                 limitaciÃ³n          \n",
            "actuales            ADJ                 amod                actual              \n",
            "en                  ADP                 case                en                  \n",
            "infraestructura     NOUN                nmod                infraestructura     \n",
            "y                   CCONJ               cc                  y                   \n",
            "software            NOUN                conj                software            \n",
            ",                   PUNCT               punct               ,                   \n",
            "y                   CCONJ               cc                  y                   \n",
            "orientada           ADJ                 conj                orientado           \n",
            "a                   ADP                 mark                a                   \n",
            "ofrecer             VERB                xcomp               ofrecer             \n",
            "escalabilidad       NOUN                obj                 escalabilidad       \n",
            ",                   PUNCT               punct               ,                   \n",
            "flexibilidad        NOUN                conj                flexibilidad        \n",
            "y                   CCONJ               cc                  y                   \n",
            "facilidad           NOUN                conj                facilidad           \n",
            "de                  ADP                 case                de                  \n",
            "mantenimiento       NOUN                nmod                mantenimiento       \n",
            ".                   PUNCT               punct               .                   \n"
          ]
        }
      ],
      "source": [
        "print(\"{:20}{:20}{:20}{:20}\".format(\"Text\", \"POS\", \"dep\", \"lemma\"))\n",
        "print(\"-----------------------------------------------------------------\")\n",
        "for token in oraciones[numero_oracion-1]:\n",
        "    print(f\"{token.text:{20}}{token.pos_:{20}}{token.dep_:{20}}{token.lemma_:{20}}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 6.7. ImplementaciÃ³n de un matcher llamado **Swimming** que encuentre las ocurrencias de una palabra escogida en el documento analizado."
      ],
      "metadata": {
        "id": "EGi_bPk9yHek"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "bScSLf__bPPU"
      },
      "outputs": [],
      "source": [
        "from spacy.matcher import Matcher       # ImportaciÃ³n de la LibrerÃ­a Matcher para realizar bÃºsqueda de una o varias palabras en un documento\n",
        "\n",
        "# Palabra a buscar en el documento analizado\n",
        "palabra = \"Inteligencia\"\n",
        "\n",
        "# Instancia de la LibrerÃ­a Matcher para realizar la bÃºsqueda\n",
        "# de la palabra seleccionada\n",
        "matcher = Matcher(nlp.vocab)\n",
        "\n",
        "# Definicion del Patron de BÃºsqueda con la Palabra definida\n",
        "pattern = [{'TEXT': palabra}]\n",
        "\n",
        "# AdiciÃ³n del patron de busqueda \"pattern\" a la Instancia matcher\n",
        "matcher.add(\"Swimming\", [pattern])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "rfUwlBP5bPPU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3908d56a-b042-495c-82c5-e4a87ed145de"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Se encontraron: 7 ocurrencias del  [{'TEXT': 'Inteligencia'}]\n",
            "Match id            Start Token         End Token           \n",
            "-----------------------------------------------------------------\n",
            "12881893835109366681                  43                  44\n",
            "12881893835109366681                 345                 346\n",
            "12881893835109366681                 436                 437\n",
            "12881893835109366681                1966                1967\n",
            "12881893835109366681                2607                2608\n",
            "12881893835109366681                3730                3731\n",
            "12881893835109366681                4144                4145\n"
          ]
        }
      ],
      "source": [
        "# AsignaciÃ³n de las ocurrencias de la palabra en el documento analizado\n",
        "found_matches = matcher(doc)\n",
        "\n",
        "# ImpresiÃ³n de las ocurrencias de la palabra en el documento analizado\n",
        "print(\"Se encontraron: {}\".format(len(found_matches)), \"ocurrencias del \", pattern )\n",
        "\n",
        "print(\"{:20}{:20}{:20}\".format(\"Match id\", \"Start Token\", \"End Token\"))\n",
        "print(\"-----------------------------------------------------------------\")\n",
        "\n",
        "# Print the details of each match\n",
        "for match_id, start, end in found_matches:\n",
        "    print(f\"{match_id:{20}}{start:{20}}{end:{20}}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 6.8. Imprimir un conjunto de palabras previas y posteriores a las Ocurrencias Encontradas en el Documento analizado"
      ],
      "metadata": {
        "id": "xraPfXUTN2yz"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "_MmR1eX4bPPU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e183c479-2272-45ce-ba4d-1c7d3ac01cde"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ocurrencia Nro.  1  :\n",
            "... de una plataforma para el despliegue de modelos de **Inteligencia** Artificial , basada en una arquitectura de microservicios .  ...\n",
            "------------------------------------------- \n",
            "\n",
            "Ocurrencia Nro.  2  :\n",
            "... fundamentada en microservicios para el despliegue de modelos de **Inteligencia** Artificial ( IA ) , desarrollados y validados previamente  ...\n",
            "------------------------------------------- \n",
            "\n",
            "Ocurrencia Nro.  3  :\n",
            "... desarrollo de backends y en el campo de la **Inteligencia** Artificial . Para la construcciÃ³n de APIs ligeras y  ...\n",
            "------------------------------------------- \n",
            "\n",
            "Ocurrencia Nro.  4  :\n",
            "... desarrollo de la plataforma de despliegue de modelos de **Inteligencia** Artificial ( IA ) , se ha adoptado una  ...\n",
            "------------------------------------------- \n",
            "\n",
            "Ocurrencia Nro.  5  :\n",
            "... de la plataforma para el despliegue de modelos de **Inteligencia** Artificial ( IA ) se ha llevado a cabo  ...\n",
            "------------------------------------------- \n",
            "\n",
            "Ocurrencia Nro.  6  :\n",
            "... de esta plataforma para el despliegue de modelos de **Inteligencia** Artificial se llevÃ³ a cabo basÃ¡ndose en la arquitectura  ...\n",
            "------------------------------------------- \n",
            "\n",
            "Ocurrencia Nro.  7  :\n",
            "... un verdadero ecosistema de operaciÃ³n continua para soluciones de **Inteligencia** Artificial . \n",
            " En conclusiÃ³n , el enfoque actual  ...\n",
            "------------------------------------------- \n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Numero de Palabras a obtener antes y despues de la Ocurrencia encontrada.\n",
        "num_palabras = 9\n",
        "\n",
        "# Iterativo para manejo de conteo de Ocurrencias en el ciclo\n",
        "num_ocurrencias = 1\n",
        "\n",
        "# Ciclo que usa el Token de Inicio y Final de la ocurrencia encontrada y\n",
        "# selecciona el numero de palabras antes y despues usando el valor de num_palabras\n",
        "for _, start, end in found_matches:\n",
        "    span = doc[start:end]\n",
        "    surrounding_span = doc[start-num_palabras:end+num_palabras]\n",
        "    highlighted_text = []\n",
        "    for token in surrounding_span:\n",
        "        if token.i >= start and token.i < end:\n",
        "            highlighted_text.append(f'**{token.text}**')\n",
        "        else:\n",
        "            highlighted_text.append(token.text)\n",
        "    print(\"Ocurrencia Nro. \", num_ocurrencias,\" :\")\n",
        "    print(\"...\",' '.join(highlighted_text),\" ...\")\n",
        "    print('------------------------------------------- \\n')\n",
        "    num_ocurrencias += 1"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 6.8. Imprimir las Oraciones con cada Ocurencia encontrada en el Documento analizado"
      ],
      "metadata": {
        "id": "9PajHHGIPzBM"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "dzAlyvDNbPPV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bbb5c908-8cc0-4ff5-b87f-e19edffde9de"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ocurrencia Nro.  1  encontrada en la oraciÃ³n: \n",
            "\n",
            "ElaboraciÃ³n de Plataforma basada en Microservicios\n",
            "Hector Yesid Castelblanco, Hector Torres , Octavio Guerra \n",
            "hector.castelblancocaro@u.icesi.edu.co  , hector.torres@u.icesi.edu.co  ,octavio.guerra@u.icesu.edu.co \n",
            "\n",
            "Resumen â€“  Este trabajo presenta el desarrollo de una plataforma para el despliegue de modelos de Inteligencia Artificial, basada en una arquitectura de microservicios. \n",
            "\n",
            "---------------------------------------------\n",
            "\n",
            "Ocurrencia Nro.  2  encontrada en la oraciÃ³n: \n",
            "\n",
            "Frente a este panorama, las arquitecturas basadas en microservicios han emergido como una alternativa sÃ³lida a los enfoques monolÃ­ticos tradicionales, permitiendo construir sistemas distribuidos, modulares y escalables con mayor flexibilidad y resiliencia.\n",
            "Este documento presenta el diseÃ±o e implementaciÃ³n de una plataforma fundamentada en microservicios para el despliegue de modelos de Inteligencia Artificial (IA), desarrollados y validados previamente, con el objetivo de optimizar su ciclo de vida e integrarlos de manera eficiente en entornos productivos. \n",
            "\n",
            "---------------------------------------------\n",
            "\n",
            "Ocurrencia Nro.  3  encontrada en la oraciÃ³n: \n",
            "\n",
            "La soluciÃ³n propuesta se basa en el lenguaje de programaciÃ³n Python, ampliamente reconocido por su versatilidad y adopciÃ³n en el desarrollo de backends y en el campo de la Inteligencia Artificial. \n",
            "\n",
            "---------------------------------------------\n",
            "\n",
            "Ocurrencia Nro.  4  encontrada en la oraciÃ³n: \n",
            "\n",
            "\n",
            "\n",
            "MetodologÃ­a\n",
            "\n",
            "Para el desarrollo de la plataforma de despliegue de modelos de Inteligencia Artificial (IA), se ha adoptado una metodologÃ­a basada en buenas prÃ¡cticas ampliamente documentadas en la industria, combinando principios del enfoque de microservicios, la filosofÃ­a 12-Factor App, y tÃ©cnicas modernas de ContenerizaciÃ³n con Docker. \n",
            "\n",
            "---------------------------------------------\n",
            "\n",
            "Ocurrencia Nro.  5  encontrada en la oraciÃ³n: \n",
            "\n",
            "Procedimientos y Herramientas Utilizadas\n",
            "\n",
            "El desarrollo de la plataforma para el despliegue de modelos de Inteligencia Artificial (IA) se ha llevado a cabo siguiendo un enfoque basado en microservicios, empleando contenedores Docker y orquestaciÃ³n con herramientas modernas. \n",
            "\n",
            "---------------------------------------------\n",
            "\n",
            "Ocurrencia Nro.  6  encontrada en la oraciÃ³n: \n",
            "\n",
            "El desarrollo de esta plataforma para el despliegue de modelos de Inteligencia Artificial se llevÃ³ a cabo basÃ¡ndose en la arquitectura de microservicios, propuesta por el propio cliente. \n",
            "\n",
            "---------------------------------------------\n",
            "\n",
            "Ocurrencia Nro.  7  encontrada en la oraciÃ³n: \n",
            "\n",
            "Esta evoluciÃ³n consolidarÃ­a la plataforma no solo como un sistema de despliegue de modelos, sino como un verdadero ecosistema de operaciÃ³n continua para soluciones de Inteligencia Artificial. \n",
            "\n",
            "---------------------------------------------\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Iterativo para el manejo de conteo de Ocurrencias en el ciclo\n",
        "num_ocurrencias = 1\n",
        "\n",
        "# Ciclo que imprime en pantalla cada OraciÃ³n que contiene\n",
        "# cada ocurrencia encontrada en el documento analizado\n",
        "for oracion in oraciones:\n",
        "    for _, start, end in found_matches:\n",
        "        if oracion.start <= start and oracion.end >= end:\n",
        "            print(\"Ocurrencia Nro. \", num_ocurrencias,\" encontrada en la oraciÃ³n: \\n\")\n",
        "            print(oracion.text, '\\n')\n",
        "            print(\"---------------------------------------------\\n\")\n",
        "            num_ocurrencias += 1"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## CONCLUSIONES"
      ],
      "metadata": {
        "id": "3avJItbGfTWE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- El uso de Spacey y un Pipeline de un Lenguaje especÃ­fico, permite hacer identificaciÃ³n del LÃ©xico en un documento cualquiera. En nuestro caso particular el uso de un Pipeline con Lenguaje EspaÃ±ol, permitio analizar un documento identificar y clasificar una variedad de palabras. Es decir, que es una herramienta que permite realizar un analisis literal, linguistico de las palabras contenidas en un texto correspondiente al idioma seleccionado y brinda a quien la usa el manejo en programaciÃ³n de la identificaciÃ³n de la variedad de palabras del lenguaje.\n",
        "\n",
        "- El anÃ¡lisis hecho del documento permitiÃ³ que SPACY a travÃ©s del Pipeline de Lenguaje en espaÃ±ol proporcionara mucho de la linguistica del texto como lo fueron: Sustantivos (noun), Adjetivos (Adj), Verbos (Verb), Conjunciones (CConj), entre otras.\n",
        "\n",
        "- El anÃ¡lisis del documento mostro un significante nÃºmero de Tokens y una amplia variedad de palabras Ãºnicas.\n",
        "\n",
        "- La palabra buscada usando \"Matcher\", \"Inteligencia\", dada la cantidad de ocurrencias, que fueron siete (7), sugiere por la cantidad encontrada y por las oraciones que las contienen, es el tema sobre el cual se desarrolla mucho del documento."
      ],
      "metadata": {
        "id": "tT37j5ATRB5Y"
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.18"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}