{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z_qR5xTubPPN"
      },
      "source": [
        "# NLP Basics Assessment"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "[![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/semurillas/NLP_MIAA_252/blob/main/Sesion%201/Practice/6-practice.ipynb)\n",
        "\n",
        "\n",
        "<center>\n",
        "\n",
        "<h1>üìö Maestr√≠a en Inteligencia Artificial Aplicada ‚Äì 3er Semestre</h1>\n",
        "\n",
        "<h3>Asignatura: Procesamiento de Lenguaje Natural</h3>\n",
        "\n",
        "<hr style=\"width:60%;\">\n",
        "\n",
        "<h2>üë®‚Äçüéì Estudiantes</h2>\n",
        "<ul style=\"list-style:none; padding:0; font-size:18px;\">\n",
        "    <li>Claudia Mart√≠nez</li>\n",
        "    <li>Sebasti√°n Murillas</li>\n",
        "    <li>Mario J. Castellanos</li>\n",
        "    <li>Enrique Manzano</li>\n",
        "    <li>Octavio Guerra</li>\n",
        "</ul>\n",
        "\n",
        "<hr style=\"width:60%;\">\n",
        "\n",
        "<h3>üìÖ Fecha: Agosto 16, 2025</h3>\n",
        "\n",
        "</center>\n"
      ],
      "metadata": {
        "id": "ovt7LOQr8Slr"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6vAnJ-Z2bPPP"
      },
      "source": [
        "En este notebook vamos a poner en pr√°ctica algunos de los conceptos vistos en los notebooks anteriores (Secci√≥n Learning en Repositorio GitHub), aplicado a un corpus espec√≠fico que cada uno de los estudiantes del grupo ha seleccionado.\n",
        "\n",
        "## Referencias\n",
        "* [NLP - Natural Language Processing With Python](https://www.udemy.com/course/nlp-natural-language-processing-with-python)\n",
        "* [Natural Language Processing in Action](https://www.manning.com/books/natural-language-processing-in-action)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Chequeo de ambiente y Manejo de \"Warnings\"\n",
        "\n",
        "En esta primer c√≥digo verificamos si el ambiente de Trabajo es \"Google Colab\" e ignorar algun mensaje de \"Warning\" que suceda a lo resto del Notebook."
      ],
      "metadata": {
        "id": "TtMCAuBEkbYH"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "wtYAN6nKbPPQ"
      },
      "outputs": [],
      "source": [
        "import importlib.metadata\n",
        "import warnings\n",
        "\n",
        "installed_packages = [dist.metadata['Name'].lower() for dist in importlib.metadata.Distribution.discover()]\n",
        "IN_COLAB = 'google-colab' in installed_packages\n",
        "\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Cargue de Librer√≠as\n",
        "\n",
        "Confirmando a traves de validaci√≥n que estamos trabajando en \"Google Colab\", instalamos las librer√≠as requeridas en el Notebook usando: **pip install -r** y el Archivo de Requerimientos desde el GitHub del Proyecto:\n",
        "\n",
        "https://raw.githubusercontent.com/semurillas/NLP_MIAA_252/refs/heads/main/requirements.txt\n",
        "\n",
        "###Notas:\n",
        "- Agregamos al URL el texto: **$(date +%s)**, para garantizar que se use la √∫ltima versi√≥n del Archivo requerimients.txt en el Notebook.\n",
        "- En el cargue con PIP esta incluida **SPACY** en el archivo requeriments.txt"
      ],
      "metadata": {
        "id": "pESu81rxld8o"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TapM8U6wbPPR",
        "outputId": "09d41c07-f4a0-40af-d3dd-3bdb6424f3fc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pip in /usr/local/lib/python3.11/dist-packages (from -r https://raw.githubusercontent.com/semurillas/NLP_MIAA_252/refs/heads/main/requirements.txt?nocache=1755386267 (line 2)) (24.1.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from -r https://raw.githubusercontent.com/semurillas/NLP_MIAA_252/refs/heads/main/requirements.txt?nocache=1755386267 (line 3)) (75.2.0)\n",
            "Requirement already satisfied: wheel in /usr/local/lib/python3.11/dist-packages (from -r https://raw.githubusercontent.com/semurillas/NLP_MIAA_252/refs/heads/main/requirements.txt?nocache=1755386267 (line 4)) (0.45.1)\n",
            "Requirement already satisfied: spacy<3.9,>=3.8 in /usr/local/lib/python3.11/dist-packages (from -r https://raw.githubusercontent.com/semurillas/NLP_MIAA_252/refs/heads/main/requirements.txt?nocache=1755386267 (line 7)) (3.8.7)\n",
            "Requirement already satisfied: nltk>=3.9 in /usr/local/lib/python3.11/dist-packages (from -r https://raw.githubusercontent.com/semurillas/NLP_MIAA_252/refs/heads/main/requirements.txt?nocache=1755386267 (line 8)) (3.9.1)\n",
            "Requirement already satisfied: transformers>=4.41 in /usr/local/lib/python3.11/dist-packages (from transformers[torch]>=4.41->-r https://raw.githubusercontent.com/semurillas/NLP_MIAA_252/refs/heads/main/requirements.txt?nocache=1755386267 (line 9)) (4.55.1)\n",
            "Requirement already satisfied: datasets>=2.19 in /usr/local/lib/python3.11/dist-packages (from -r https://raw.githubusercontent.com/semurillas/NLP_MIAA_252/refs/heads/main/requirements.txt?nocache=1755386267 (line 10)) (4.0.0)\n",
            "Requirement already satisfied: sentence-transformers>=3.0 in /usr/local/lib/python3.11/dist-packages (from -r https://raw.githubusercontent.com/semurillas/NLP_MIAA_252/refs/heads/main/requirements.txt?nocache=1755386267 (line 11)) (5.1.0)\n",
            "Requirement already satisfied: torch>=2.6 in /usr/local/lib/python3.11/dist-packages (from -r https://raw.githubusercontent.com/semurillas/NLP_MIAA_252/refs/heads/main/requirements.txt?nocache=1755386267 (line 14)) (2.6.0+cu124)\n",
            "Requirement already satisfied: torchvision>=0.21 in /usr/local/lib/python3.11/dist-packages (from -r https://raw.githubusercontent.com/semurillas/NLP_MIAA_252/refs/heads/main/requirements.txt?nocache=1755386267 (line 15)) (0.21.0+cu124)\n",
            "Requirement already satisfied: torchaudio>=2.6 in /usr/local/lib/python3.11/dist-packages (from -r https://raw.githubusercontent.com/semurillas/NLP_MIAA_252/refs/heads/main/requirements.txt?nocache=1755386267 (line 16)) (2.6.0+cu124)\n",
            "Collecting lightning>=2.2 (from -r https://raw.githubusercontent.com/semurillas/NLP_MIAA_252/refs/heads/main/requirements.txt?nocache=1755386267 (line 17))\n",
            "  Downloading lightning-2.5.3-py3-none-any.whl.metadata (39 kB)\n",
            "Requirement already satisfied: tensorboard==2.19.0 in /usr/local/lib/python3.11/dist-packages (from -r https://raw.githubusercontent.com/semurillas/NLP_MIAA_252/refs/heads/main/requirements.txt?nocache=1755386267 (line 18)) (2.19.0)\n",
            "Requirement already satisfied: accelerate>=0.30 in /usr/local/lib/python3.11/dist-packages (from -r https://raw.githubusercontent.com/semurillas/NLP_MIAA_252/refs/heads/main/requirements.txt?nocache=1755386267 (line 19)) (1.10.0)\n",
            "Collecting evaluate>=0.4 (from -r https://raw.githubusercontent.com/semurillas/NLP_MIAA_252/refs/heads/main/requirements.txt?nocache=1755386267 (line 20))\n",
            "  Downloading evaluate-0.4.5-py3-none-any.whl.metadata (9.5 kB)\n",
            "Requirement already satisfied: pandas>=2.2 in /usr/local/lib/python3.11/dist-packages (from -r https://raw.githubusercontent.com/semurillas/NLP_MIAA_252/refs/heads/main/requirements.txt?nocache=1755386267 (line 23)) (2.2.2)\n",
            "Requirement already satisfied: matplotlib>=3.8 in /usr/local/lib/python3.11/dist-packages (from -r https://raw.githubusercontent.com/semurillas/NLP_MIAA_252/refs/heads/main/requirements.txt?nocache=1755386267 (line 24)) (3.10.0)\n",
            "Requirement already satisfied: seaborn>=0.12 in /usr/local/lib/python3.11/dist-packages (from -r https://raw.githubusercontent.com/semurillas/NLP_MIAA_252/refs/heads/main/requirements.txt?nocache=1755386267 (line 25)) (0.13.2)\n",
            "Requirement already satisfied: statsmodels>=0.14 in /usr/local/lib/python3.11/dist-packages (from -r https://raw.githubusercontent.com/semurillas/NLP_MIAA_252/refs/heads/main/requirements.txt?nocache=1755386267 (line 26)) (0.14.5)\n",
            "Requirement already satisfied: tqdm>=4.67 in /usr/local/lib/python3.11/dist-packages (from -r https://raw.githubusercontent.com/semurillas/NLP_MIAA_252/refs/heads/main/requirements.txt?nocache=1755386267 (line 27)) (4.67.1)\n",
            "Requirement already satisfied: bokeh>=3.5 in /usr/local/lib/python3.11/dist-packages (from -r https://raw.githubusercontent.com/semurillas/NLP_MIAA_252/refs/heads/main/requirements.txt?nocache=1755386267 (line 28)) (3.7.3)\n",
            "Requirement already satisfied: gradio>=4.36 in /usr/local/lib/python3.11/dist-packages (from -r https://raw.githubusercontent.com/semurillas/NLP_MIAA_252/refs/heads/main/requirements.txt?nocache=1755386267 (line 31)) (5.42.0)\n",
            "Collecting ollama>=0.2 (from -r https://raw.githubusercontent.com/semurillas/NLP_MIAA_252/refs/heads/main/requirements.txt?nocache=1755386267 (line 32))\n",
            "  Downloading ollama-0.5.3-py3-none-any.whl.metadata (4.3 kB)\n",
            "Requirement already satisfied: websockets>=14.0 in /usr/local/lib/python3.11/dist-packages (from -r https://raw.githubusercontent.com/semurillas/NLP_MIAA_252/refs/heads/main/requirements.txt?nocache=1755386267 (line 33)) (15.0.1)\n",
            "Collecting python-docx (from -r https://raw.githubusercontent.com/semurillas/NLP_MIAA_252/refs/heads/main/requirements.txt?nocache=1755386267 (line 34))\n",
            "  Downloading python_docx-1.2.0-py3-none-any.whl.metadata (2.0 kB)\n",
            "Requirement already satisfied: httpx<1.0.0,>=0.28.1 in /usr/local/lib/python3.11/dist-packages (from -r https://raw.githubusercontent.com/semurillas/NLP_MIAA_252/refs/heads/main/requirements.txt?nocache=1755386267 (line 35)) (0.28.1)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.11/dist-packages (from tensorboard==2.19.0->-r https://raw.githubusercontent.com/semurillas/NLP_MIAA_252/refs/heads/main/requirements.txt?nocache=1755386267 (line 18)) (1.4.0)\n",
            "Requirement already satisfied: grpcio>=1.48.2 in /usr/local/lib/python3.11/dist-packages (from tensorboard==2.19.0->-r https://raw.githubusercontent.com/semurillas/NLP_MIAA_252/refs/heads/main/requirements.txt?nocache=1755386267 (line 18)) (1.74.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.11/dist-packages (from tensorboard==2.19.0->-r https://raw.githubusercontent.com/semurillas/NLP_MIAA_252/refs/heads/main/requirements.txt?nocache=1755386267 (line 18)) (3.8.2)\n",
            "Requirement already satisfied: numpy>=1.12.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard==2.19.0->-r https://raw.githubusercontent.com/semurillas/NLP_MIAA_252/refs/heads/main/requirements.txt?nocache=1755386267 (line 18)) (2.0.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from tensorboard==2.19.0->-r https://raw.githubusercontent.com/semurillas/NLP_MIAA_252/refs/heads/main/requirements.txt?nocache=1755386267 (line 18)) (25.0)\n",
            "Requirement already satisfied: protobuf!=4.24.0,>=3.19.6 in /usr/local/lib/python3.11/dist-packages (from tensorboard==2.19.0->-r https://raw.githubusercontent.com/semurillas/NLP_MIAA_252/refs/heads/main/requirements.txt?nocache=1755386267 (line 18)) (5.29.5)\n",
            "Requirement already satisfied: six>1.9 in /usr/local/lib/python3.11/dist-packages (from tensorboard==2.19.0->-r https://raw.githubusercontent.com/semurillas/NLP_MIAA_252/refs/heads/main/requirements.txt?nocache=1755386267 (line 18)) (1.17.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard==2.19.0->-r https://raw.githubusercontent.com/semurillas/NLP_MIAA_252/refs/heads/main/requirements.txt?nocache=1755386267 (line 18)) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from tensorboard==2.19.0->-r https://raw.githubusercontent.com/semurillas/NLP_MIAA_252/refs/heads/main/requirements.txt?nocache=1755386267 (line 18)) (3.1.3)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.11/dist-packages (from spacy<3.9,>=3.8->-r https://raw.githubusercontent.com/semurillas/NLP_MIAA_252/refs/heads/main/requirements.txt?nocache=1755386267 (line 7)) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.9,>=3.8->-r https://raw.githubusercontent.com/semurillas/NLP_MIAA_252/refs/heads/main/requirements.txt?nocache=1755386267 (line 7)) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.9,>=3.8->-r https://raw.githubusercontent.com/semurillas/NLP_MIAA_252/refs/heads/main/requirements.txt?nocache=1755386267 (line 7)) (1.0.13)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.11/dist-packages (from spacy<3.9,>=3.8->-r https://raw.githubusercontent.com/semurillas/NLP_MIAA_252/refs/heads/main/requirements.txt?nocache=1755386267 (line 7)) (2.0.11)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.11/dist-packages (from spacy<3.9,>=3.8->-r https://raw.githubusercontent.com/semurillas/NLP_MIAA_252/refs/heads/main/requirements.txt?nocache=1755386267 (line 7)) (3.0.10)\n",
            "Requirement already satisfied: thinc<8.4.0,>=8.3.4 in /usr/local/lib/python3.11/dist-packages (from spacy<3.9,>=3.8->-r https://raw.githubusercontent.com/semurillas/NLP_MIAA_252/refs/heads/main/requirements.txt?nocache=1755386267 (line 7)) (8.3.6)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.11/dist-packages (from spacy<3.9,>=3.8->-r https://raw.githubusercontent.com/semurillas/NLP_MIAA_252/refs/heads/main/requirements.txt?nocache=1755386267 (line 7)) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.11/dist-packages (from spacy<3.9,>=3.8->-r https://raw.githubusercontent.com/semurillas/NLP_MIAA_252/refs/heads/main/requirements.txt?nocache=1755386267 (line 7)) (2.5.1)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.11/dist-packages (from spacy<3.9,>=3.8->-r https://raw.githubusercontent.com/semurillas/NLP_MIAA_252/refs/heads/main/requirements.txt?nocache=1755386267 (line 7)) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.9,>=3.8->-r https://raw.githubusercontent.com/semurillas/NLP_MIAA_252/refs/heads/main/requirements.txt?nocache=1755386267 (line 7)) (0.4.1)\n",
            "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.9,>=3.8->-r https://raw.githubusercontent.com/semurillas/NLP_MIAA_252/refs/heads/main/requirements.txt?nocache=1755386267 (line 7)) (0.16.0)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.9,>=3.8->-r https://raw.githubusercontent.com/semurillas/NLP_MIAA_252/refs/heads/main/requirements.txt?nocache=1755386267 (line 7)) (2.32.3)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.11/dist-packages (from spacy<3.9,>=3.8->-r https://raw.githubusercontent.com/semurillas/NLP_MIAA_252/refs/heads/main/requirements.txt?nocache=1755386267 (line 7)) (2.11.7)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from spacy<3.9,>=3.8->-r https://raw.githubusercontent.com/semurillas/NLP_MIAA_252/refs/heads/main/requirements.txt?nocache=1755386267 (line 7)) (3.1.6)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.9,>=3.8->-r https://raw.githubusercontent.com/semurillas/NLP_MIAA_252/refs/heads/main/requirements.txt?nocache=1755386267 (line 7)) (3.5.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk>=3.9->-r https://raw.githubusercontent.com/semurillas/NLP_MIAA_252/refs/heads/main/requirements.txt?nocache=1755386267 (line 8)) (8.2.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk>=3.9->-r https://raw.githubusercontent.com/semurillas/NLP_MIAA_252/refs/heads/main/requirements.txt?nocache=1755386267 (line 8)) (1.5.1)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk>=3.9->-r https://raw.githubusercontent.com/semurillas/NLP_MIAA_252/refs/heads/main/requirements.txt?nocache=1755386267 (line 8)) (2024.11.6)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers>=4.41->transformers[torch]>=4.41->-r https://raw.githubusercontent.com/semurillas/NLP_MIAA_252/refs/heads/main/requirements.txt?nocache=1755386267 (line 9)) (3.18.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.41->transformers[torch]>=4.41->-r https://raw.githubusercontent.com/semurillas/NLP_MIAA_252/refs/heads/main/requirements.txt?nocache=1755386267 (line 9)) (0.34.4)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.41->transformers[torch]>=4.41->-r https://raw.githubusercontent.com/semurillas/NLP_MIAA_252/refs/heads/main/requirements.txt?nocache=1755386267 (line 9)) (6.0.2)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.41->transformers[torch]>=4.41->-r https://raw.githubusercontent.com/semurillas/NLP_MIAA_252/refs/heads/main/requirements.txt?nocache=1755386267 (line 9)) (0.21.4)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.41->transformers[torch]>=4.41->-r https://raw.githubusercontent.com/semurillas/NLP_MIAA_252/refs/heads/main/requirements.txt?nocache=1755386267 (line 9)) (0.6.2)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.19->-r https://raw.githubusercontent.com/semurillas/NLP_MIAA_252/refs/heads/main/requirements.txt?nocache=1755386267 (line 10)) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.19->-r https://raw.githubusercontent.com/semurillas/NLP_MIAA_252/refs/heads/main/requirements.txt?nocache=1755386267 (line 10)) (0.3.8)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets>=2.19->-r https://raw.githubusercontent.com/semurillas/NLP_MIAA_252/refs/heads/main/requirements.txt?nocache=1755386267 (line 10)) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.19->-r https://raw.githubusercontent.com/semurillas/NLP_MIAA_252/refs/heads/main/requirements.txt?nocache=1755386267 (line 10)) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=2.19->-r https://raw.githubusercontent.com/semurillas/NLP_MIAA_252/refs/heads/main/requirements.txt?nocache=1755386267 (line 10)) (2025.3.0)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (from sentence-transformers>=3.0->-r https://raw.githubusercontent.com/semurillas/NLP_MIAA_252/refs/heads/main/requirements.txt?nocache=1755386267 (line 11)) (1.6.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from sentence-transformers>=3.0->-r https://raw.githubusercontent.com/semurillas/NLP_MIAA_252/refs/heads/main/requirements.txt?nocache=1755386267 (line 11)) (1.16.1)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.11/dist-packages (from sentence-transformers>=3.0->-r https://raw.githubusercontent.com/semurillas/NLP_MIAA_252/refs/heads/main/requirements.txt?nocache=1755386267 (line 11)) (11.3.0)\n",
            "Requirement already satisfied: typing_extensions>=4.5.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers>=3.0->-r https://raw.githubusercontent.com/semurillas/NLP_MIAA_252/refs/heads/main/requirements.txt?nocache=1755386267 (line 11)) (4.14.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=2.6->-r https://raw.githubusercontent.com/semurillas/NLP_MIAA_252/refs/heads/main/requirements.txt?nocache=1755386267 (line 14)) (3.5)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch>=2.6->-r https://raw.githubusercontent.com/semurillas/NLP_MIAA_252/refs/heads/main/requirements.txt?nocache=1755386267 (line 14))\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch>=2.6->-r https://raw.githubusercontent.com/semurillas/NLP_MIAA_252/refs/heads/main/requirements.txt?nocache=1755386267 (line 14))\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch>=2.6->-r https://raw.githubusercontent.com/semurillas/NLP_MIAA_252/refs/heads/main/requirements.txt?nocache=1755386267 (line 14))\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch>=2.6->-r https://raw.githubusercontent.com/semurillas/NLP_MIAA_252/refs/heads/main/requirements.txt?nocache=1755386267 (line 14))\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch>=2.6->-r https://raw.githubusercontent.com/semurillas/NLP_MIAA_252/refs/heads/main/requirements.txt?nocache=1755386267 (line 14))\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch>=2.6->-r https://raw.githubusercontent.com/semurillas/NLP_MIAA_252/refs/heads/main/requirements.txt?nocache=1755386267 (line 14))\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch>=2.6->-r https://raw.githubusercontent.com/semurillas/NLP_MIAA_252/refs/heads/main/requirements.txt?nocache=1755386267 (line 14))\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch>=2.6->-r https://raw.githubusercontent.com/semurillas/NLP_MIAA_252/refs/heads/main/requirements.txt?nocache=1755386267 (line 14))\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch>=2.6->-r https://raw.githubusercontent.com/semurillas/NLP_MIAA_252/refs/heads/main/requirements.txt?nocache=1755386267 (line 14))\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.6->-r https://raw.githubusercontent.com/semurillas/NLP_MIAA_252/refs/heads/main/requirements.txt?nocache=1755386267 (line 14)) (0.6.2)\n",
            "Collecting nvidia-nccl-cu12==2.21.5 (from torch>=2.6->-r https://raw.githubusercontent.com/semurillas/NLP_MIAA_252/refs/heads/main/requirements.txt?nocache=1755386267 (line 14))\n",
            "  Downloading nvidia_nccl_cu12-2.21.5-py3-none-manylinux2014_x86_64.whl.metadata (1.8 kB)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.6->-r https://raw.githubusercontent.com/semurillas/NLP_MIAA_252/refs/heads/main/requirements.txt?nocache=1755386267 (line 14)) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch>=2.6->-r https://raw.githubusercontent.com/semurillas/NLP_MIAA_252/refs/heads/main/requirements.txt?nocache=1755386267 (line 14))\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=2.6->-r https://raw.githubusercontent.com/semurillas/NLP_MIAA_252/refs/heads/main/requirements.txt?nocache=1755386267 (line 14)) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=2.6->-r https://raw.githubusercontent.com/semurillas/NLP_MIAA_252/refs/heads/main/requirements.txt?nocache=1755386267 (line 14)) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=2.6->-r https://raw.githubusercontent.com/semurillas/NLP_MIAA_252/refs/heads/main/requirements.txt?nocache=1755386267 (line 14)) (1.3.0)\n",
            "Collecting lightning-utilities<2.0,>=0.10.0 (from lightning>=2.2->-r https://raw.githubusercontent.com/semurillas/NLP_MIAA_252/refs/heads/main/requirements.txt?nocache=1755386267 (line 17))\n",
            "  Downloading lightning_utilities-0.15.2-py3-none-any.whl.metadata (5.7 kB)\n",
            "Collecting torchmetrics<3.0,>0.7.0 (from lightning>=2.2->-r https://raw.githubusercontent.com/semurillas/NLP_MIAA_252/refs/heads/main/requirements.txt?nocache=1755386267 (line 17))\n",
            "  Downloading torchmetrics-1.8.1-py3-none-any.whl.metadata (22 kB)\n",
            "Collecting pytorch-lightning (from lightning>=2.2->-r https://raw.githubusercontent.com/semurillas/NLP_MIAA_252/refs/heads/main/requirements.txt?nocache=1755386267 (line 17))\n",
            "  Downloading pytorch_lightning-2.5.3-py3-none-any.whl.metadata (20 kB)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from accelerate>=0.30->-r https://raw.githubusercontent.com/semurillas/NLP_MIAA_252/refs/heads/main/requirements.txt?nocache=1755386267 (line 19)) (5.9.5)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas>=2.2->-r https://raw.githubusercontent.com/semurillas/NLP_MIAA_252/refs/heads/main/requirements.txt?nocache=1755386267 (line 23)) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=2.2->-r https://raw.githubusercontent.com/semurillas/NLP_MIAA_252/refs/heads/main/requirements.txt?nocache=1755386267 (line 23)) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=2.2->-r https://raw.githubusercontent.com/semurillas/NLP_MIAA_252/refs/heads/main/requirements.txt?nocache=1755386267 (line 23)) (2025.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.8->-r https://raw.githubusercontent.com/semurillas/NLP_MIAA_252/refs/heads/main/requirements.txt?nocache=1755386267 (line 24)) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.8->-r https://raw.githubusercontent.com/semurillas/NLP_MIAA_252/refs/heads/main/requirements.txt?nocache=1755386267 (line 24)) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.8->-r https://raw.githubusercontent.com/semurillas/NLP_MIAA_252/refs/heads/main/requirements.txt?nocache=1755386267 (line 24)) (4.59.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.8->-r https://raw.githubusercontent.com/semurillas/NLP_MIAA_252/refs/heads/main/requirements.txt?nocache=1755386267 (line 24)) (1.4.9)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.8->-r https://raw.githubusercontent.com/semurillas/NLP_MIAA_252/refs/heads/main/requirements.txt?nocache=1755386267 (line 24)) (3.2.3)\n",
            "Requirement already satisfied: patsy>=0.5.6 in /usr/local/lib/python3.11/dist-packages (from statsmodels>=0.14->-r https://raw.githubusercontent.com/semurillas/NLP_MIAA_252/refs/heads/main/requirements.txt?nocache=1755386267 (line 26)) (1.0.1)\n",
            "Requirement already satisfied: narwhals>=1.13 in /usr/local/lib/python3.11/dist-packages (from bokeh>=3.5->-r https://raw.githubusercontent.com/semurillas/NLP_MIAA_252/refs/heads/main/requirements.txt?nocache=1755386267 (line 28)) (2.1.1)\n",
            "Requirement already satisfied: tornado>=6.2 in /usr/local/lib/python3.11/dist-packages (from bokeh>=3.5->-r https://raw.githubusercontent.com/semurillas/NLP_MIAA_252/refs/heads/main/requirements.txt?nocache=1755386267 (line 28)) (6.4.2)\n",
            "Requirement already satisfied: xyzservices>=2021.09.1 in /usr/local/lib/python3.11/dist-packages (from bokeh>=3.5->-r https://raw.githubusercontent.com/semurillas/NLP_MIAA_252/refs/heads/main/requirements.txt?nocache=1755386267 (line 28)) (2025.4.0)\n",
            "Requirement already satisfied: aiofiles<25.0,>=22.0 in /usr/local/lib/python3.11/dist-packages (from gradio>=4.36->-r https://raw.githubusercontent.com/semurillas/NLP_MIAA_252/refs/heads/main/requirements.txt?nocache=1755386267 (line 31)) (24.1.0)\n",
            "Requirement already satisfied: anyio<5.0,>=3.0 in /usr/local/lib/python3.11/dist-packages (from gradio>=4.36->-r https://raw.githubusercontent.com/semurillas/NLP_MIAA_252/refs/heads/main/requirements.txt?nocache=1755386267 (line 31)) (4.10.0)\n",
            "Requirement already satisfied: brotli>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from gradio>=4.36->-r https://raw.githubusercontent.com/semurillas/NLP_MIAA_252/refs/heads/main/requirements.txt?nocache=1755386267 (line 31)) (1.1.0)\n",
            "Requirement already satisfied: fastapi<1.0,>=0.115.2 in /usr/local/lib/python3.11/dist-packages (from gradio>=4.36->-r https://raw.githubusercontent.com/semurillas/NLP_MIAA_252/refs/heads/main/requirements.txt?nocache=1755386267 (line 31)) (0.116.1)\n",
            "Requirement already satisfied: ffmpy in /usr/local/lib/python3.11/dist-packages (from gradio>=4.36->-r https://raw.githubusercontent.com/semurillas/NLP_MIAA_252/refs/heads/main/requirements.txt?nocache=1755386267 (line 31)) (0.6.1)\n",
            "Requirement already satisfied: gradio-client==1.11.1 in /usr/local/lib/python3.11/dist-packages (from gradio>=4.36->-r https://raw.githubusercontent.com/semurillas/NLP_MIAA_252/refs/heads/main/requirements.txt?nocache=1755386267 (line 31)) (1.11.1)\n",
            "Requirement already satisfied: groovy~=0.1 in /usr/local/lib/python3.11/dist-packages (from gradio>=4.36->-r https://raw.githubusercontent.com/semurillas/NLP_MIAA_252/refs/heads/main/requirements.txt?nocache=1755386267 (line 31)) (0.1.2)\n",
            "Requirement already satisfied: markupsafe<4.0,>=2.0 in /usr/local/lib/python3.11/dist-packages (from gradio>=4.36->-r https://raw.githubusercontent.com/semurillas/NLP_MIAA_252/refs/heads/main/requirements.txt?nocache=1755386267 (line 31)) (3.0.2)\n",
            "Requirement already satisfied: orjson~=3.0 in /usr/local/lib/python3.11/dist-packages (from gradio>=4.36->-r https://raw.githubusercontent.com/semurillas/NLP_MIAA_252/refs/heads/main/requirements.txt?nocache=1755386267 (line 31)) (3.11.2)\n",
            "Requirement already satisfied: pydub in /usr/local/lib/python3.11/dist-packages (from gradio>=4.36->-r https://raw.githubusercontent.com/semurillas/NLP_MIAA_252/refs/heads/main/requirements.txt?nocache=1755386267 (line 31)) (0.25.1)\n",
            "Requirement already satisfied: python-multipart>=0.0.18 in /usr/local/lib/python3.11/dist-packages (from gradio>=4.36->-r https://raw.githubusercontent.com/semurillas/NLP_MIAA_252/refs/heads/main/requirements.txt?nocache=1755386267 (line 31)) (0.0.20)\n",
            "Requirement already satisfied: ruff>=0.9.3 in /usr/local/lib/python3.11/dist-packages (from gradio>=4.36->-r https://raw.githubusercontent.com/semurillas/NLP_MIAA_252/refs/heads/main/requirements.txt?nocache=1755386267 (line 31)) (0.12.8)\n",
            "Requirement already satisfied: safehttpx<0.2.0,>=0.1.6 in /usr/local/lib/python3.11/dist-packages (from gradio>=4.36->-r https://raw.githubusercontent.com/semurillas/NLP_MIAA_252/refs/heads/main/requirements.txt?nocache=1755386267 (line 31)) (0.1.6)\n",
            "Requirement already satisfied: semantic-version~=2.0 in /usr/local/lib/python3.11/dist-packages (from gradio>=4.36->-r https://raw.githubusercontent.com/semurillas/NLP_MIAA_252/refs/heads/main/requirements.txt?nocache=1755386267 (line 31)) (2.10.0)\n",
            "Requirement already satisfied: starlette<1.0,>=0.40.0 in /usr/local/lib/python3.11/dist-packages (from gradio>=4.36->-r https://raw.githubusercontent.com/semurillas/NLP_MIAA_252/refs/heads/main/requirements.txt?nocache=1755386267 (line 31)) (0.47.2)\n",
            "Requirement already satisfied: tomlkit<0.14.0,>=0.12.0 in /usr/local/lib/python3.11/dist-packages (from gradio>=4.36->-r https://raw.githubusercontent.com/semurillas/NLP_MIAA_252/refs/heads/main/requirements.txt?nocache=1755386267 (line 31)) (0.13.3)\n",
            "Requirement already satisfied: uvicorn>=0.14.0 in /usr/local/lib/python3.11/dist-packages (from gradio>=4.36->-r https://raw.githubusercontent.com/semurillas/NLP_MIAA_252/refs/heads/main/requirements.txt?nocache=1755386267 (line 31)) (0.35.0)\n",
            "Requirement already satisfied: lxml>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from python-docx->-r https://raw.githubusercontent.com/semurillas/NLP_MIAA_252/refs/heads/main/requirements.txt?nocache=1755386267 (line 34)) (5.4.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx<1.0.0,>=0.28.1->-r https://raw.githubusercontent.com/semurillas/NLP_MIAA_252/refs/heads/main/requirements.txt?nocache=1755386267 (line 35)) (2025.8.3)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1.0.0,>=0.28.1->-r https://raw.githubusercontent.com/semurillas/NLP_MIAA_252/refs/heads/main/requirements.txt?nocache=1755386267 (line 35)) (1.0.9)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from httpx<1.0.0,>=0.28.1->-r https://raw.githubusercontent.com/semurillas/NLP_MIAA_252/refs/heads/main/requirements.txt?nocache=1755386267 (line 35)) (3.10)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1.0.0,>=0.28.1->-r https://raw.githubusercontent.com/semurillas/NLP_MIAA_252/refs/heads/main/requirements.txt?nocache=1755386267 (line 35)) (0.16.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio<5.0,>=3.0->gradio>=4.36->-r https://raw.githubusercontent.com/semurillas/NLP_MIAA_252/refs/heads/main/requirements.txt?nocache=1755386267 (line 31)) (1.3.1)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=2.19->-r https://raw.githubusercontent.com/semurillas/NLP_MIAA_252/refs/heads/main/requirements.txt?nocache=1755386267 (line 10)) (3.12.15)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers>=4.41->transformers[torch]>=4.41->-r https://raw.githubusercontent.com/semurillas/NLP_MIAA_252/refs/heads/main/requirements.txt?nocache=1755386267 (line 9)) (1.1.7)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.11/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy<3.9,>=3.8->-r https://raw.githubusercontent.com/semurillas/NLP_MIAA_252/refs/heads/main/requirements.txt?nocache=1755386267 (line 7)) (1.3.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.9,>=3.8->-r https://raw.githubusercontent.com/semurillas/NLP_MIAA_252/refs/heads/main/requirements.txt?nocache=1755386267 (line 7)) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.9,>=3.8->-r https://raw.githubusercontent.com/semurillas/NLP_MIAA_252/refs/heads/main/requirements.txt?nocache=1755386267 (line 7)) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.9,>=3.8->-r https://raw.githubusercontent.com/semurillas/NLP_MIAA_252/refs/heads/main/requirements.txt?nocache=1755386267 (line 7)) (0.4.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.9,>=3.8->-r https://raw.githubusercontent.com/semurillas/NLP_MIAA_252/refs/heads/main/requirements.txt?nocache=1755386267 (line 7)) (3.4.3)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.9,>=3.8->-r https://raw.githubusercontent.com/semurillas/NLP_MIAA_252/refs/heads/main/requirements.txt?nocache=1755386267 (line 7)) (2.5.0)\n",
            "Requirement already satisfied: blis<1.4.0,>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from thinc<8.4.0,>=8.3.4->spacy<3.9,>=3.8->-r https://raw.githubusercontent.com/semurillas/NLP_MIAA_252/refs/heads/main/requirements.txt?nocache=1755386267 (line 7)) (1.3.0)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.11/dist-packages (from thinc<8.4.0,>=8.3.4->spacy<3.9,>=3.8->-r https://raw.githubusercontent.com/semurillas/NLP_MIAA_252/refs/heads/main/requirements.txt?nocache=1755386267 (line 7)) (0.1.5)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.9,>=3.8->-r https://raw.githubusercontent.com/semurillas/NLP_MIAA_252/refs/heads/main/requirements.txt?nocache=1755386267 (line 7)) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.9,>=3.8->-r https://raw.githubusercontent.com/semurillas/NLP_MIAA_252/refs/heads/main/requirements.txt?nocache=1755386267 (line 7)) (13.9.4)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from weasel<0.5.0,>=0.1.0->spacy<3.9,>=3.8->-r https://raw.githubusercontent.com/semurillas/NLP_MIAA_252/refs/heads/main/requirements.txt?nocache=1755386267 (line 7)) (0.21.1)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.11/dist-packages (from weasel<0.5.0,>=0.1.0->spacy<3.9,>=3.8->-r https://raw.githubusercontent.com/semurillas/NLP_MIAA_252/refs/heads/main/requirements.txt?nocache=1755386267 (line 7)) (7.3.0.post1)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->sentence-transformers>=3.0->-r https://raw.githubusercontent.com/semurillas/NLP_MIAA_252/refs/heads/main/requirements.txt?nocache=1755386267 (line 11)) (3.6.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=2.19->-r https://raw.githubusercontent.com/semurillas/NLP_MIAA_252/refs/heads/main/requirements.txt?nocache=1755386267 (line 10)) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=2.19->-r https://raw.githubusercontent.com/semurillas/NLP_MIAA_252/refs/heads/main/requirements.txt?nocache=1755386267 (line 10)) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=2.19->-r https://raw.githubusercontent.com/semurillas/NLP_MIAA_252/refs/heads/main/requirements.txt?nocache=1755386267 (line 10)) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=2.19->-r https://raw.githubusercontent.com/semurillas/NLP_MIAA_252/refs/heads/main/requirements.txt?nocache=1755386267 (line 10)) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=2.19->-r https://raw.githubusercontent.com/semurillas/NLP_MIAA_252/refs/heads/main/requirements.txt?nocache=1755386267 (line 10)) (6.6.4)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=2.19->-r https://raw.githubusercontent.com/semurillas/NLP_MIAA_252/refs/heads/main/requirements.txt?nocache=1755386267 (line 10)) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=2.19->-r https://raw.githubusercontent.com/semurillas/NLP_MIAA_252/refs/heads/main/requirements.txt?nocache=1755386267 (line 10)) (1.20.1)\n",
            "Requirement already satisfied: marisa-trie>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy<3.9,>=3.8->-r https://raw.githubusercontent.com/semurillas/NLP_MIAA_252/refs/heads/main/requirements.txt?nocache=1755386267 (line 7)) (1.2.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.9,>=3.8->-r https://raw.githubusercontent.com/semurillas/NLP_MIAA_252/refs/heads/main/requirements.txt?nocache=1755386267 (line 7)) (4.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.9,>=3.8->-r https://raw.githubusercontent.com/semurillas/NLP_MIAA_252/refs/heads/main/requirements.txt?nocache=1755386267 (line 7)) (2.19.2)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy<3.9,>=3.8->-r https://raw.githubusercontent.com/semurillas/NLP_MIAA_252/refs/heads/main/requirements.txt?nocache=1755386267 (line 7)) (1.17.3)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.9,>=3.8->-r https://raw.githubusercontent.com/semurillas/NLP_MIAA_252/refs/heads/main/requirements.txt?nocache=1755386267 (line 7)) (0.1.2)\n",
            "Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m76.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m60.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m40.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m753.1 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m11.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nccl_cu12-2.21.5-py3-none-manylinux2014_x86_64.whl (188.7 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m188.7/188.7 MB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m63.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading lightning-2.5.3-py3-none-any.whl (824 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m824.2/824.2 kB\u001b[0m \u001b[31m26.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading evaluate-0.4.5-py3-none-any.whl (84 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m84.1/84.1 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ollama-0.5.3-py3-none-any.whl (13 kB)\n",
            "Downloading python_docx-1.2.0-py3-none-any.whl (252 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m253.0/253.0 kB\u001b[0m \u001b[31m13.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading lightning_utilities-0.15.2-py3-none-any.whl (29 kB)\n",
            "Downloading torchmetrics-1.8.1-py3-none-any.whl (982 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m983.0/983.0 kB\u001b[0m \u001b[31m23.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pytorch_lightning-2.5.3-py3-none-any.whl (828 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m828.2/828.2 kB\u001b[0m \u001b[31m22.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: python-docx, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, lightning-utilities, nvidia-cusparse-cu12, nvidia-cudnn-cu12, ollama, nvidia-cusolver-cu12, torchmetrics, evaluate, pytorch-lightning, lightning\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-nccl-cu12\n",
            "    Found existing installation: nvidia-nccl-cu12 2.23.4\n",
            "    Uninstalling nvidia-nccl-cu12-2.23.4:\n",
            "      Successfully uninstalled nvidia-nccl-cu12-2.23.4\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed evaluate-0.4.5 lightning-2.5.3 lightning-utilities-0.15.2 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nccl-cu12-2.21.5 nvidia-nvjitlink-cu12-12.4.127 ollama-0.5.3 python-docx-1.2.0 pytorch-lightning-2.5.3 torchmetrics-1.8.1\n"
          ]
        }
      ],
      "source": [
        "!test '{IN_COLAB}' = 'True' && pip install -r \"https://raw.githubusercontent.com/semurillas/NLP_MIAA_252/refs/heads/main/requirements.txt?nocache=$(date +%s)\""
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Descarga del Pipeline de Idioma a usar Con SPACY\n",
        "\n",
        "Decidimos usar el Pipeline entrenado de Idioma Espa√±ol: **es_dep_news_trf** (https://spacy.io/models/es)\n",
        "\n",
        "El c√≥digo a continuaci√≥n lo descarga para su uso con **SPACY**"
      ],
      "metadata": {
        "id": "XnJWKts_mwHH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m spacy download es_dep_news_trf"
      ],
      "metadata": {
        "id": "vQpXXAHDlROI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ec587424-f696-40be-989e-1e033f622533"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting es-dep-news-trf==3.8.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/es_dep_news_trf-3.8.0/es_dep_news_trf-3.8.0-py3-none-any.whl (407.8 MB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m407.8/407.8 MB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting spacy-curated-transformers<1.0.0,>=0.2.2 (from es-dep-news-trf==3.8.0)\n",
            "  Downloading spacy_curated_transformers-0.3.1-py2.py3-none-any.whl.metadata (2.7 kB)\n",
            "Collecting curated-transformers<0.2.0,>=0.1.0 (from spacy-curated-transformers<1.0.0,>=0.2.2->es-dep-news-trf==3.8.0)\n",
            "  Downloading curated_transformers-0.1.1-py2.py3-none-any.whl.metadata (965 bytes)\n",
            "Collecting curated-tokenizers<0.1.0,>=0.0.9 (from spacy-curated-transformers<1.0.0,>=0.2.2->es-dep-news-trf==3.8.0)\n",
            "  Downloading curated_tokenizers-0.0.9-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.9 kB)\n",
            "Requirement already satisfied: torch>=1.12.0 in /usr/local/lib/python3.11/dist-packages (from spacy-curated-transformers<1.0.0,>=0.2.2->es-dep-news-trf==3.8.0) (2.6.0+cu124)\n",
            "Requirement already satisfied: regex>=2022 in /usr/local/lib/python3.11/dist-packages (from curated-tokenizers<0.1.0,>=0.0.9->spacy-curated-transformers<1.0.0,>=0.2.2->es-dep-news-trf==3.8.0) (2024.11.6)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch>=1.12.0->spacy-curated-transformers<1.0.0,>=0.2.2->es-dep-news-trf==3.8.0) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.12.0->spacy-curated-transformers<1.0.0,>=0.2.2->es-dep-news-trf==3.8.0) (4.14.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.12.0->spacy-curated-transformers<1.0.0,>=0.2.2->es-dep-news-trf==3.8.0) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.12.0->spacy-curated-transformers<1.0.0,>=0.2.2->es-dep-news-trf==3.8.0) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch>=1.12.0->spacy-curated-transformers<1.0.0,>=0.2.2->es-dep-news-trf==3.8.0) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.12.0->spacy-curated-transformers<1.0.0,>=0.2.2->es-dep-news-trf==3.8.0) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.12.0->spacy-curated-transformers<1.0.0,>=0.2.2->es-dep-news-trf==3.8.0) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.12.0->spacy-curated-transformers<1.0.0,>=0.2.2->es-dep-news-trf==3.8.0) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=1.12.0->spacy-curated-transformers<1.0.0,>=0.2.2->es-dep-news-trf==3.8.0) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=1.12.0->spacy-curated-transformers<1.0.0,>=0.2.2->es-dep-news-trf==3.8.0) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=1.12.0->spacy-curated-transformers<1.0.0,>=0.2.2->es-dep-news-trf==3.8.0) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=1.12.0->spacy-curated-transformers<1.0.0,>=0.2.2->es-dep-news-trf==3.8.0) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=1.12.0->spacy-curated-transformers<1.0.0,>=0.2.2->es-dep-news-trf==3.8.0) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=1.12.0->spacy-curated-transformers<1.0.0,>=0.2.2->es-dep-news-trf==3.8.0) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.12.0->spacy-curated-transformers<1.0.0,>=0.2.2->es-dep-news-trf==3.8.0) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.12.0->spacy-curated-transformers<1.0.0,>=0.2.2->es-dep-news-trf==3.8.0) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.12.0->spacy-curated-transformers<1.0.0,>=0.2.2->es-dep-news-trf==3.8.0) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.12.0->spacy-curated-transformers<1.0.0,>=0.2.2->es-dep-news-trf==3.8.0) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.12.0->spacy-curated-transformers<1.0.0,>=0.2.2->es-dep-news-trf==3.8.0) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.12.0->spacy-curated-transformers<1.0.0,>=0.2.2->es-dep-news-trf==3.8.0) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.12.0->spacy-curated-transformers<1.0.0,>=0.2.2->es-dep-news-trf==3.8.0) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.12.0->spacy-curated-transformers<1.0.0,>=0.2.2->es-dep-news-trf==3.8.0) (3.0.2)\n",
            "Downloading spacy_curated_transformers-0.3.1-py2.py3-none-any.whl (237 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m237.9/237.9 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading curated_tokenizers-0.0.9-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (735 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m735.6/735.6 kB\u001b[0m \u001b[31m20.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading curated_transformers-0.1.1-py2.py3-none-any.whl (25 kB)\n",
            "Installing collected packages: curated-tokenizers, curated-transformers, spacy-curated-transformers, es-dep-news-trf\n",
            "Successfully installed curated-tokenizers-0.0.9 curated-transformers-0.1.1 es-dep-news-trf-3.8.0 spacy-curated-transformers-0.3.1\n",
            "\u001b[38;5;2m‚úî Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('es_dep_news_trf')\n",
            "\u001b[38;5;3m‚ö† Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Nota 1**\n",
        "\n",
        "Por favor, recuerde realizar\n",
        "\n",
        "**Men√∫ superior --> Runtime --> Restart runtime**\n",
        "\n",
        "Antes de continuar con los siguientes pasos."
      ],
      "metadata": {
        "id": "pwmsydGvo-lf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Cargue del Pipeline Seleccionado a una Instancia de NLP\n",
        "\n",
        "Con la descarga del Pipeline entrenado del idioma Espa√±ol **es_dep_news_trf** que se seleccion√≥ lo \"cargamos\" a una instancia NLP de la librer√≠a **SPACY**, para poder hacer procesamiento de alg√∫n texto en idioma **Espa√±ol**"
      ],
      "metadata": {
        "id": "4l9Dzod4nY3U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "nlp = spacy.load('es_dep_news_trf')"
      ],
      "metadata": {
        "id": "V99A4u3mZK8R"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5. An√°lisis de Texto con Instancia de SPACY y Pipeline es_dep_news_trf\n",
        "\n",
        "Vamos a usar **SPACY** con el Pipeline Entrenado que hemos seleccionado **es_dep_news_trf** (https://spacy.io/models/es)\n",
        "\n",
        "Seleccionamos un texto correspondiente a un documento **Word** denominado *Proyecto II de Innovaci√≥n Tecnol√≥gica*.\n",
        "\n",
        "Dado que el archivo est√° en formato Word (**.docx**), debemos primero convertirlo a un texto antes de su procesamiento por **SPACY**"
      ],
      "metadata": {
        "id": "fP-JOPvoplMA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import docx           # Libreria requerida para trabajar con documentos Word\n",
        "import requests       # Libreria requerida para hacer HTTP requests\n",
        "\n",
        "# Paso 1: Descargar el documento del Proyecto de GitHub en la ruta especificada.\n",
        "url = \"https://raw.githubusercontent.com/semurillas/NLP_MIAA_252/main/Texts/Proyecto%20-%20Despliegue%20Modelos%20IA%20-%20Microservicios%20-%20Informe%20Final%20v3.docx\"\n",
        "docx_file_path = \"informe_final.docx\"\n",
        "\n",
        "response = requests.get(url)\n",
        "with open(docx_file_path, \"wb\") as f:\n",
        "    f.write(response.content)\n",
        "\n",
        "print(f\"Archivo descargado como: {docx_file_path} \\n\")     # Confirmaci√≥n de que el archivo seleccionado haya sido descargado exitosamente.\n",
        "\n",
        "# Paso 2: Funci√≥n para leer el archivo Word descargado y llevar su contenido a formato texto\n",
        "def read_docx(file_path):\n",
        "    \"\"\"Reads a .docx file and returns its text content.\"\"\"\n",
        "    doc = docx.Document(file_path)\n",
        "    all_text = []\n",
        "    for paragraph in doc.paragraphs:\n",
        "        all_text.append(paragraph.text)\n",
        "    return '\\n'.join(all_text)\n",
        "\n",
        "# Paso 3: Ejecuci√≥n de la Funci√≥n read_docx() para transformar el contenido del archivo Word a Texto\n",
        "text_content = read_docx(docx_file_path)\n",
        "\n",
        "# Paso 4: Procesamiento del archivo texto obtenido con la instancia NLP basada en \"SPACY\"\n",
        "doc = nlp(text_content)\n",
        "print(docx_file_path,' has been processed with NLP Spacy model succesfully \\n')\n"
      ],
      "metadata": {
        "id": "4eCDCBQ9m9CK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "021331af-671f-4d9f-e7e4-cdf74e8bcc1e"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archivo descargado como: informe_final.docx \n",
            "\n",
            "informe_final.docx  has been processed with NLP model succesfully \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6. An√°lisis del Documento procesado con SPACY y Lenguaje Espa√±ol\n",
        "\n",
        "Usando el documento procesado por NLP basado en **SPACY**, vamos a obtener:\n",
        "- Cantidad de Tokens en el documento analizado\n",
        "- Cantidad de Tokens √önicos en el documento Analizado\n",
        "- Muestra de una cantidad de Tokens √önicos\n",
        "- Contar y Mostrar los Tokens mas usados en el documento analizado\n",
        "- Oraciones encontradas en el documento analizado\n",
        "- Mostrar (Imprimir) alguna Oraci√≥n seleccionada del documento analizado\n",
        "- Mostrar Text, POS, dep y Lemma de los Tokens en la Oraci√≥n seleccionada\n",
        "- Uso de Matcher con un Patr√≥n definido y encontrar cantidad de ocurrencias en el documento analizado.\n",
        "- Mostrar palabras previas y posteriores a las Ocurrencias Encontradas en el documento analizado\n",
        "- Mostrar las Oraciones que contienen las Ocurrencias Encontradas en el documento analizado."
      ],
      "metadata": {
        "id": "zmxmleu4qiuk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 6.1. Cantidad de Tokens en el Documento Analizado"
      ],
      "metadata": {
        "id": "RR4ssxtlroa7"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "PC6KlqG-bPPT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "52dcefed-adde-49d9-8a90-6bf6fb82cfaf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "El documento analizado contiene:  4932 Tokens\n"
          ]
        }
      ],
      "source": [
        "print(\"El documento analizado contiene: \", len(doc), \"Tokens\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 6.2. Cantidad de Tokens √önicos en el Documento Analizado"
      ],
      "metadata": {
        "id": "NH11B0hlsHXP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "unique_tokens = set([token.text for token in doc])\n",
        "print(f\"El documento analizado contiene :\", len(unique_tokens), \"Tokens √∫nicos\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gXfijkC_edip",
        "outputId": "9e4f4c55-75e5-42a7-bf19-a673e61bd4dd"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "El documento analizado contiene : 1218 Tokens √∫nicos\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 6.3. Muestra de Tokens √önicos en el Documento Analizado"
      ],
      "metadata": {
        "id": "t8Zq8oossc9V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Immpresi√≥n de Encabezado en Pantalla\n",
        "print(\"{:20}{:20}{:20}{:20}\".format(\"Text\", \"POS\", \"dep\", \"lemma\"))\n",
        "print(\"-----------------------------------------------------------------\")\n",
        "\n",
        "# Selecci√≥n de Cantidad de Tokens √önicos a Mostrar\n",
        "cant_unique_tokens = 20\n",
        "\n",
        "# Ciclo para imprimir en Pantalla la cantidad de Tokens √önicos seleccionados\n",
        "for token in list(unique_tokens)[:cant_unique_tokens]:\n",
        "    # Encuentra la primera ocurrencia del Token √önico en el documento\n",
        "    for t in doc:\n",
        "        if t.text == token:\n",
        "            print(f\"{t.text:{20}}{t.pos_:{20}}{t.dep_:{20}}{t.lemma_:{20}}\")\n",
        "            break # Nos movemos al siguiente Token √önico"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vgIHvmAweuZa",
        "outputId": "22b0e164-d9d7-45e2-a6b3-27525b99fea8"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Text                POS                 dep                 lemma               \n",
            "-----------------------------------------------------------------\n",
            "enfoque             NOUN                obl                 enfoque             \n",
            "Biomedical          PROPN               flat                Biomedical          \n",
            "frameworks          NOUN                nsubj               framework           \n",
            "open                PROPN               flat                open                \n",
            "modernas            ADJ                 amod                moderno             \n",
            "Ap√©ndice            PROPN               flat                Ap√©ndice            \n",
            "etapas              NOUN                obl                 etapa               \n",
            "robusta             ADJ                 amod                robusto             \n",
            "referencia          NOUN                nmod                referencia          \n",
            "Bajo                ADP                 case                bajo                \n",
            "basadas             ADJ                 amod                basado              \n",
            "programaci√≥n        NOUN                nmod                programaci√≥n        \n",
            "usando              VERB                advcl               usar                \n",
            "escenarios          NOUN                nmod                escenario           \n",
            "Damiani             PROPN               flat                Damiani             \n",
            "estandarizada       ADJ                 amod                estandarizado       \n",
            "OpenShift           PROPN               conj                OpenShift           \n",
            "soluciones          NOUN                nmod                soluci√≥n            \n",
            "opcional            ADJ                 amod                opcional            \n",
            "Su                  DET                 det                 su                  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 6.4. Contar y Mostrar los Tokens m√°s Usados en el Documento Analizado"
      ],
      "metadata": {
        "id": "e2kL7fcvuCL2"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bd192d7f",
        "outputId": "6cac0c57-1995-4e5a-a2c5-fca01409c432"
      },
      "source": [
        "from collections import Counter\n",
        "\n",
        "# Removemos puntuaci√≥n y palabras \"stop\" para el analisis de frecuencia de Tokens\n",
        "filtered_tokens = [token.text for token in doc if not token.is_punct and not token.is_stop]\n",
        "\n",
        "# Conteo de la frecuencia de cada Token\n",
        "token_counts = Counter(filtered_tokens)\n",
        "\n",
        "# Mostrar en Pantalla los 20 Tokens mas encontrados (Mayor a Menor)\n",
        "print(\"Tokens mas encontrados (excluyendo puntuaci√≥n y palabras stop):\\n\")\n",
        "print(\"{:20}{:20}{:20}\".format(\"Token\", \"||\", \"Cantidad\"))\n",
        "print(\"-------------------------------------------------\")\n",
        "\n",
        "for token, count in token_counts.most_common(20):\n",
        "    print(\"{:20}{:20}{:10}\".format(token, \"||\", count))"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokens mas encontrados (excluyendo puntuaci√≥n y palabras stop):\n",
            "\n",
            "Token               ||                  Cantidad            \n",
            "-------------------------------------------------\n",
            "\n",
            "                   ||                         129\n",
            "microservicios      ||                          42\n",
            "despliegue          ||                          26\n",
            "\n",
            "\n",
            "                  ||                          25\n",
            "Docker              ||                          23\n",
            "modelos             ||                          22\n",
            "desarrollo          ||                          20\n",
            "IA                  ||                          19\n",
            "and                 ||                          17\n",
            "plataforma          ||                          16\n",
            "contenedores        ||                          16\n",
            "entornos            ||                          15\n",
            "sistema             ||                          14\n",
            "microservicio       ||                          13\n",
            "servicios           ||                          13\n",
            "Microservices       ||                          13\n",
            "API                 ||                          12\n",
            "enfoque             ||                          12\n",
            "CI                  ||                          12\n",
            "CD                  ||                          12\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 6.5. Conteo de Oraciones en el Documento Analizado"
      ],
      "metadata": {
        "id": "QGrQVnqDv_5d"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "UObgeX6FbPPT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bc2c869b-833d-437b-b5f4-b7c4ed2dfa37"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "El documento analizado tiene:  214 oraciones\n"
          ]
        }
      ],
      "source": [
        "oraciones = list(doc.sents)\n",
        "print(\"El documento analizado tiene: \", len(oraciones), \"oraciones\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 6.6. Imprimir una de las Oraciones en el Documento Analizado"
      ],
      "metadata": {
        "id": "yDBhO4-LwUVV"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "Hh7xnToDbPPT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5780c585-c166-44ad-f7fb-33dea9db95fb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "La oraci√≥n Nro. 3 es: \n",
            " La soluci√≥n fue dise√±ada en respuesta a la propuesta y necesidades del cliente, considerando sus limitaciones actuales en infraestructura y software, y orientada a ofrecer escalabilidad, flexibilidad y facilidad de mantenimiento.\n"
          ]
        }
      ],
      "source": [
        "# Numero de Oraci√≥n a imprimir\n",
        "numero_oracion = 3\n",
        "\n",
        "# Chequeo de que el N√∫mero de Oraci√≥n este en el rango de la cantidad encontrada\n",
        "# en el documento analizado\n",
        "if numero_oracion > 0 & numero_oracion <= len(oraciones):\n",
        "    print(\"La oraci√≥n Nro.\", numero_oracion, \"es: \\n\", oraciones[numero_oracion-1])\n",
        "else:\n",
        "    print(f\"No hay una oraci√≥n con el n√∫mero {numero_oracion} en el documento analizado\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 6.7. Imprimir Token, text, POS tag, dep tag y lemma de la Oraci√≥n seleccionada en el paso 6.6."
      ],
      "metadata": {
        "id": "B0600kOixmr-"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "fscVIMCYbPPU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8e038880-8bb8-47ed-c6f8-ba5cce96fa6b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Text                POS                 dep                 lemma               \n",
            "-----------------------------------------------------------------\n",
            "La                  DET                 det                 el                  \n",
            "soluci√≥n            NOUN                nsubj               soluci√≥n            \n",
            "fue                 AUX                 aux                 ser                 \n",
            "dise√±ada            VERB                ROOT                dise√±ar             \n",
            "en                  ADP                 case                en                  \n",
            "respuesta           NOUN                obl                 respuesta           \n",
            "a                   ADP                 case                a                   \n",
            "la                  DET                 det                 el                  \n",
            "propuesta           NOUN                nmod                propuesta           \n",
            "y                   CCONJ               cc                  y                   \n",
            "necesidades         NOUN                conj                necesidad           \n",
            "del                 ADP                 case                del                 \n",
            "cliente             NOUN                nmod                cliente             \n",
            ",                   PUNCT               punct               ,                   \n",
            "considerando        VERB                advcl               considerar          \n",
            "sus                 DET                 det                 su                  \n",
            "limitaciones        NOUN                obj                 limitaci√≥n          \n",
            "actuales            ADJ                 amod                actual              \n",
            "en                  ADP                 case                en                  \n",
            "infraestructura     NOUN                nmod                infraestructura     \n",
            "y                   CCONJ               cc                  y                   \n",
            "software            NOUN                conj                software            \n",
            ",                   PUNCT               punct               ,                   \n",
            "y                   CCONJ               cc                  y                   \n",
            "orientada           ADJ                 conj                orientado           \n",
            "a                   ADP                 mark                a                   \n",
            "ofrecer             VERB                xcomp               ofrecer             \n",
            "escalabilidad       NOUN                obj                 escalabilidad       \n",
            ",                   PUNCT               punct               ,                   \n",
            "flexibilidad        NOUN                conj                flexibilidad        \n",
            "y                   CCONJ               cc                  y                   \n",
            "facilidad           NOUN                conj                facilidad           \n",
            "de                  ADP                 case                de                  \n",
            "mantenimiento       NOUN                nmod                mantenimiento       \n",
            ".                   PUNCT               punct               .                   \n"
          ]
        }
      ],
      "source": [
        "print(\"{:20}{:20}{:20}{:20}\".format(\"Text\", \"POS\", \"dep\", \"lemma\"))\n",
        "print(\"-----------------------------------------------------------------\")\n",
        "for token in oraciones[numero_oracion-1]:\n",
        "    print(f\"{token.text:{20}}{token.pos_:{20}}{token.dep_:{20}}{token.lemma_:{20}}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 6.7. Implementaci√≥n de un matcher llamado **Swimming** que encuentre las ocurrencias de una palabra escogida en el documento analizado."
      ],
      "metadata": {
        "id": "EGi_bPk9yHek"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "bScSLf__bPPU"
      },
      "outputs": [],
      "source": [
        "from spacy.matcher import Matcher       # Importaci√≥n de la Librer√≠a Matcher para realizar b√∫squeda de una o varias palabras en un documento\n",
        "\n",
        "# Palabra a buscar en el documento analizado\n",
        "palabra = \"Inteligencia\"\n",
        "\n",
        "# Instancia de la Librer√≠a Matcher para realizar la b√∫squeda\n",
        "# de la palabra seleccionada\n",
        "matcher = Matcher(nlp.vocab)\n",
        "\n",
        "# Definicion del Patron de B√∫squeda con la Palabra definida\n",
        "pattern = [{'TEXT': palabra}]\n",
        "\n",
        "# Adici√≥n del patron de busqueda \"pattern\" a la Instancia matcher\n",
        "matcher.add(\"Swimming\", [pattern])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "rfUwlBP5bPPU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3908d56a-b042-495c-82c5-e4a87ed145de"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Se encontraron: 7 ocurrencias del  [{'TEXT': 'Inteligencia'}]\n",
            "Match id            Start Token         End Token           \n",
            "-----------------------------------------------------------------\n",
            "12881893835109366681                  43                  44\n",
            "12881893835109366681                 345                 346\n",
            "12881893835109366681                 436                 437\n",
            "12881893835109366681                1966                1967\n",
            "12881893835109366681                2607                2608\n",
            "12881893835109366681                3730                3731\n",
            "12881893835109366681                4144                4145\n"
          ]
        }
      ],
      "source": [
        "# Asignaci√≥n de las ocurrencias de la palabra en el documento analizado\n",
        "found_matches = matcher(doc)\n",
        "\n",
        "# Impresi√≥n de las ocurrencias de la palabra en el documento analizado\n",
        "print(\"Se encontraron: {}\".format(len(found_matches)), \"ocurrencias del \", pattern )\n",
        "\n",
        "print(\"{:20}{:20}{:20}\".format(\"Match id\", \"Start Token\", \"End Token\"))\n",
        "print(\"-----------------------------------------------------------------\")\n",
        "\n",
        "# Print the details of each match\n",
        "for match_id, start, end in found_matches:\n",
        "    print(f\"{match_id:{20}}{start:{20}}{end:{20}}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 6.8. Imprimir un conjunto de palabras previas y posteriores a las Ocurrencias Encontradas en el Documento analizado"
      ],
      "metadata": {
        "id": "xraPfXUTN2yz"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "_MmR1eX4bPPU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e183c479-2272-45ce-ba4d-1c7d3ac01cde"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ocurrencia Nro.  1  :\n",
            "... de una plataforma para el despliegue de modelos de **Inteligencia** Artificial , basada en una arquitectura de microservicios .  ...\n",
            "------------------------------------------- \n",
            "\n",
            "Ocurrencia Nro.  2  :\n",
            "... fundamentada en microservicios para el despliegue de modelos de **Inteligencia** Artificial ( IA ) , desarrollados y validados previamente  ...\n",
            "------------------------------------------- \n",
            "\n",
            "Ocurrencia Nro.  3  :\n",
            "... desarrollo de backends y en el campo de la **Inteligencia** Artificial . Para la construcci√≥n de APIs ligeras y  ...\n",
            "------------------------------------------- \n",
            "\n",
            "Ocurrencia Nro.  4  :\n",
            "... desarrollo de la plataforma de despliegue de modelos de **Inteligencia** Artificial ( IA ) , se ha adoptado una  ...\n",
            "------------------------------------------- \n",
            "\n",
            "Ocurrencia Nro.  5  :\n",
            "... de la plataforma para el despliegue de modelos de **Inteligencia** Artificial ( IA ) se ha llevado a cabo  ...\n",
            "------------------------------------------- \n",
            "\n",
            "Ocurrencia Nro.  6  :\n",
            "... de esta plataforma para el despliegue de modelos de **Inteligencia** Artificial se llev√≥ a cabo bas√°ndose en la arquitectura  ...\n",
            "------------------------------------------- \n",
            "\n",
            "Ocurrencia Nro.  7  :\n",
            "... un verdadero ecosistema de operaci√≥n continua para soluciones de **Inteligencia** Artificial . \n",
            " En conclusi√≥n , el enfoque actual  ...\n",
            "------------------------------------------- \n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Numero de Palabras a obtener antes y despues de la Ocurrencia encontrada.\n",
        "num_palabras = 9\n",
        "\n",
        "# Iterativo para manejo de conteo de Ocurrencias en el ciclo\n",
        "num_ocurrencias = 1\n",
        "\n",
        "# Ciclo que usa el Token de Inicio y Final de la ocurrencia encontrada y\n",
        "# selecciona el numero de palabras antes y despues usando el valor de num_palabras\n",
        "for _, start, end in found_matches:\n",
        "    span = doc[start:end]\n",
        "    surrounding_span = doc[start-num_palabras:end+num_palabras]\n",
        "    highlighted_text = []\n",
        "    for token in surrounding_span:\n",
        "        if token.i >= start and token.i < end:\n",
        "            highlighted_text.append(f'**{token.text}**')\n",
        "        else:\n",
        "            highlighted_text.append(token.text)\n",
        "    print(\"Ocurrencia Nro. \", num_ocurrencias,\" :\")\n",
        "    print(\"...\",' '.join(highlighted_text),\" ...\")\n",
        "    print('------------------------------------------- \\n')\n",
        "    num_ocurrencias += 1"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 6.8. Imprimir las Oraciones con cada Ocurencia encontrada en el Documento analizado"
      ],
      "metadata": {
        "id": "9PajHHGIPzBM"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "dzAlyvDNbPPV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bbb5c908-8cc0-4ff5-b87f-e19edffde9de"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ocurrencia Nro.  1  encontrada en la oraci√≥n: \n",
            "\n",
            "Elaboraci√≥n de Plataforma basada en Microservicios\n",
            "Hector Yesid Castelblanco, Hector Torres , Octavio Guerra \n",
            "hector.castelblancocaro@u.icesi.edu.co  , hector.torres@u.icesi.edu.co  ,octavio.guerra@u.icesu.edu.co \n",
            "\n",
            "Resumen ‚Äì  Este trabajo presenta el desarrollo de una plataforma para el despliegue de modelos de Inteligencia Artificial, basada en una arquitectura de microservicios. \n",
            "\n",
            "---------------------------------------------\n",
            "\n",
            "Ocurrencia Nro.  2  encontrada en la oraci√≥n: \n",
            "\n",
            "Frente a este panorama, las arquitecturas basadas en microservicios han emergido como una alternativa s√≥lida a los enfoques monol√≠ticos tradicionales, permitiendo construir sistemas distribuidos, modulares y escalables con mayor flexibilidad y resiliencia.\n",
            "Este documento presenta el dise√±o e implementaci√≥n de una plataforma fundamentada en microservicios para el despliegue de modelos de Inteligencia Artificial (IA), desarrollados y validados previamente, con el objetivo de optimizar su ciclo de vida e integrarlos de manera eficiente en entornos productivos. \n",
            "\n",
            "---------------------------------------------\n",
            "\n",
            "Ocurrencia Nro.  3  encontrada en la oraci√≥n: \n",
            "\n",
            "La soluci√≥n propuesta se basa en el lenguaje de programaci√≥n Python, ampliamente reconocido por su versatilidad y adopci√≥n en el desarrollo de backends y en el campo de la Inteligencia Artificial. \n",
            "\n",
            "---------------------------------------------\n",
            "\n",
            "Ocurrencia Nro.  4  encontrada en la oraci√≥n: \n",
            "\n",
            "\n",
            "\n",
            "Metodolog√≠a\n",
            "\n",
            "Para el desarrollo de la plataforma de despliegue de modelos de Inteligencia Artificial (IA), se ha adoptado una metodolog√≠a basada en buenas pr√°cticas ampliamente documentadas en la industria, combinando principios del enfoque de microservicios, la filosof√≠a 12-Factor App, y t√©cnicas modernas de Contenerizaci√≥n con Docker. \n",
            "\n",
            "---------------------------------------------\n",
            "\n",
            "Ocurrencia Nro.  5  encontrada en la oraci√≥n: \n",
            "\n",
            "Procedimientos y Herramientas Utilizadas\n",
            "\n",
            "El desarrollo de la plataforma para el despliegue de modelos de Inteligencia Artificial (IA) se ha llevado a cabo siguiendo un enfoque basado en microservicios, empleando contenedores Docker y orquestaci√≥n con herramientas modernas. \n",
            "\n",
            "---------------------------------------------\n",
            "\n",
            "Ocurrencia Nro.  6  encontrada en la oraci√≥n: \n",
            "\n",
            "El desarrollo de esta plataforma para el despliegue de modelos de Inteligencia Artificial se llev√≥ a cabo bas√°ndose en la arquitectura de microservicios, propuesta por el propio cliente. \n",
            "\n",
            "---------------------------------------------\n",
            "\n",
            "Ocurrencia Nro.  7  encontrada en la oraci√≥n: \n",
            "\n",
            "Esta evoluci√≥n consolidar√≠a la plataforma no solo como un sistema de despliegue de modelos, sino como un verdadero ecosistema de operaci√≥n continua para soluciones de Inteligencia Artificial. \n",
            "\n",
            "---------------------------------------------\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Iterativo para el manejo de conteo de Ocurrencias en el ciclo\n",
        "num_ocurrencias = 1\n",
        "\n",
        "# Ciclo que imprime en pantalla cada Oraci√≥n que contiene\n",
        "# cada ocurrencia encontrada en el documento analizado\n",
        "for oracion in oraciones:\n",
        "    for _, start, end in found_matches:\n",
        "        if oracion.start <= start and oracion.end >= end:\n",
        "            print(\"Ocurrencia Nro. \", num_ocurrencias,\" encontrada en la oraci√≥n: \\n\")\n",
        "            print(oracion.text, '\\n')\n",
        "            print(\"---------------------------------------------\\n\")\n",
        "            num_ocurrencias += 1"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## CONCLUSIONES"
      ],
      "metadata": {
        "id": "3avJItbGfTWE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- El uso de Spacey y un Pipeline de un Lenguaje espec√≠fico, permite hacer identificaci√≥n del L√©xico en un documento cualquiera. En nuestro caso particular el uso de un Pipeline con Lenguaje Espa√±ol, permitio analizar un documento identificar y clasificar una variedad de palabras. Es decir, que es una herramienta que permite realizar un analisis literal, linguistico de las palabras contenidas en un texto correspondiente al idioma seleccionado y brinda a quien la usa el manejo en programaci√≥n de la identificaci√≥n de la variedad de palabras del lenguaje.\n",
        "\n",
        "- El an√°lisis hecho del documento permiti√≥ que SPACY a trav√©s del Pipeline de Lenguaje en espa√±ol proporcionara mucho de la linguistica del texto como lo fueron: Sustantivos (noun), Adjetivos (Adj), Verbos (Verb), Conjunciones (CConj), entre otras.\n",
        "\n",
        "- El an√°lisis del documento mostro un significante n√∫mero de Tokens y una amplia variedad de palabras √∫nicas.\n",
        "\n",
        "- La palabra buscada usando \"Matcher\", \"Inteligencia\", dada la cantidad de ocurrencias, que fueron siete (7), sugiere por la cantidad encontrada y por las oraciones que las contienen, es el tema sobre el cual se desarrolla mucho del documento."
      ],
      "metadata": {
        "id": "tT37j5ATRB5Y"
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.18"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}